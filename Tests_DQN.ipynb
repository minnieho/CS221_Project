{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Anti Collision Tests openai gym environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT (Anti Collision Tests) with 10 cars using cv driver model\n",
      "SEED 15438151168752304802\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_act\n",
    "\n",
    "# Our Anti Collision Tests environement\n",
    "env = gym.make(\"Act-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def show_img(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN network: start experiments with a simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs=44, outputs=5):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to conver numpy arrays to torch arrays \n",
    "# On GPU if possible, with floats 32 bits\n",
    "def numpy_to_torch(state):\n",
    "    s = torch.from_numpy(state).to(device)\n",
    "    # unsqueeze(0) to add a batch dim\n",
    "    s = s.unsqueeze(0).to(device).float()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "action = 0\n",
    "obs, reward, done, info = env.step(action)\n",
    "env.reset()\n",
    "img = env.render()\n",
    "show_img(img)\n",
    "\n",
    "n_feats = env.observation_space.shape[0] # ego [x,y,vx,vy] + 10 cars [x,y,vx,vy]\n",
    "assert n_feats == 44 # should be 44 ...\n",
    "\n",
    "policy_net = DQN(n_feats, n_actions).to(device)\n",
    "target_net = DQN(n_feats, n_actions).to(device)\n",
    "\n",
    "policy_net = policy_net.float()\n",
    "target_net = target_net.float()\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(state)).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    print(\"loss {}\".format(loss))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training example (on just a few episodes)\n",
    "\n",
    "Some usefull references to follow up ideas and code:\n",
    "* Playing Atari with Deep Reinforcement Learning, V. Mnih et al., NIPS Workshop, 2013:\n",
    "  https://arxiv.org/abs/1312.5602  \n",
    "* Human-level control through deep reinforcement learning, V. Mnih et al., Nature, 2015:  \n",
    "  https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning  \n",
    "* Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning, Carl-Johan Hoel et al., ITSC, 2018:  \n",
    "  https://arxiv.org/abs/1803.10056  \n",
    "* Pytorch dqn starter code: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html  \n",
    "\n",
    "We typically have to train for 10 million steps.  \n",
    "But in this notebook, as an illustration, we just run for around 1000 steps.  \n",
    "We confirm that this problem is much more difficult to solve than eg cartpole.  \n",
    "And is not trivially solved or learned in a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 8: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 14: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 16: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 17: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 18: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 19: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 20: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 22: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 24: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 26: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 27: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 28: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 29: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 30: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 31: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 32: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 33: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 34: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 35: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 37: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 39: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 40: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 41: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 43: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 47: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 48: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 0 with cumulated_reward 950\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 3: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 7: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 14: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 16: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 17: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 19: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 22: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 24: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 25: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 26: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 27: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 28: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 29: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 30: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 31: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 32: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 33: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 34: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 35: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 36: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 37: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 39: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 40: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 41: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 42: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 43: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 44: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 46: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 50: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 51: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 52: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 53: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 1 with cumulated_reward 946\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 2: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 2 with cumulated_reward -1014\n",
      "done!\n",
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 6: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 29.373708724975586\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 1336.9052734375\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 35.66288757324219\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 28.70933723449707\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 43.01341247558594\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 36.19375991821289\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 22.709022521972656\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 22.534870147705078\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 21.10540008544922\n",
      "Step 18: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 28.924686431884766\n",
      "Step 19: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 28.98604393005371\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 26.013145446777344\n",
      "Step 21: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.584007263183594\n",
      "Step 22: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 19.71440887451172\n",
      "Step 23: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 28.637948989868164\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 19.73346519470215\n",
      "Step 25: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 3 with cumulated_reward -1026\n",
      "loss 33.79412078857422\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 33.951717376708984\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 34.011756896972656\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.84203338623047\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 35.45830154418945\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 34.70982360839844\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.23237228393555\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 19.01512336730957\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.665681838989258\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.60625076293945\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 34.64080810546875\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.48247146606445\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.75837326049805\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.41853713989258\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.823015213012695\n",
      "Step 14: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.571041107177734\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.803253173828125\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 18.074193954467773\n",
      "Step 17: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.252384185791016\n",
      "Step 18: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.725284576416016\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.97371292114258\n",
      "Step 20: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.867717742919922\n",
      "Step 21: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.228702545166016\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.246482849121094\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.05978012084961\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.442968368530273\n",
      "Step 25: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.16701889038086\n",
      "Step 26: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.343711853027344\n",
      "Step 27: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.7120361328125\n",
      "Step 28: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.290340423583984\n",
      "Step 29: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.33711624145508\n",
      "Step 30: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.683996200561523\n",
      "Step 31: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 1.601675033569336\n",
      "Step 32: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.01414489746094\n",
      "Step 33: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.425159454345703\n",
      "Step 34: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.147310256958008\n",
      "Step 35: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.43345260620117\n",
      "Step 36: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.339725494384766\n",
      "Step 37: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.376800537109375\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.03415298461914\n",
      "Step 39: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.39144515991211\n",
      "Step 40: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.9827880859375\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.734867095947266\n",
      "Step 42: action=tensor([[0]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 4 with cumulated_reward -1043\n",
      "loss 32.778079986572266\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.365535736083984\n",
      "Step 1: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.65065002441406\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.621858596801758\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.79711151123047\n",
      "Step 4: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.475128173828125\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.08466339111328\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.09381866455078\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.075939178466797\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.01520538330078\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.30645751953125\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.825368881225586\n",
      "Step 11: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.33074951171875\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.6965274810791\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.236169815063477\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.18575668334961\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.842599868774414\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.047810554504395\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.055529594421387\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.83079719543457\n",
      "Step 19: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.740055084228516\n",
      "Step 20: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.97654914855957\n",
      "Step 21: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.257465362548828\n",
      "Step 22: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 5 with cumulated_reward -1023\n",
      "loss 16.750307083129883\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.270931243896484\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.22040557861328\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.29167938232422\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.32292938232422\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.70253562927246\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.917816162109375\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.5662841796875\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.983705520629883\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.20685577392578\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.4266357421875\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.80709457397461\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.433053970336914\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.62929916381836\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.497774124145508\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.51289939880371\n",
      "Step 15: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.607833862304688\n",
      "Step 16: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.66645050048828\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.34462356567383\n",
      "Step 18: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.753782272338867\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.13638687133789\n",
      "Step 20: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.038254737854004\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.436735153198242\n",
      "Step 22: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.015275955200195\n",
      "Step 23: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.821168899536133\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.79762077331543\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.04692077636719\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.4108829498291\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.910655975341797\n",
      "Step 28: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.429744720458984\n",
      "Step 29: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.974714279174805\n",
      "Step 30: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.6304988861084\n",
      "Step 31: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.822007179260254\n",
      "Step 32: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.296220779418945\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.560707092285156\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.43568992614746\n",
      "Step 35: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.97519874572754\n",
      "Step 36: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.726091384887695\n",
      "Step 37: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.186336517333984\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.353050231933594\n",
      "Step 39: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.40748596191406\n",
      "Step 40: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.729507446289062\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.665094375610352\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.13843536376953\n",
      "Step 43: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.176101684570312\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.406232833862305\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.558218002319336\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.79500961303711\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.36427879333496\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 6 with cumulated_reward 951\n",
      "loss 31.963926315307617\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.83409118652344\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.264816284179688\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 34.257896423339844\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.55941390991211\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.132917404174805\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.15509796142578\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.21413803100586\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.693540573120117\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.89398193359375\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.94464111328125\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.603331565856934\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.187694549560547\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.916479110717773\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.474618911743164\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.283287048339844\n",
      "Step 15: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.03132629394531\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.28648376464844\n",
      "Step 17: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.105072021484375\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.05048370361328\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.615432739257812\n",
      "Step 20: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.685081481933594\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.446842193603516\n",
      "Step 22: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.4141845703125\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.154605865478516\n",
      "Step 24: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 7 with cumulated_reward -1025\n",
      "loss 16.773944854736328\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.91740417480469\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 1.1166507005691528\n",
      "Step 2: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.782169342041016\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.54692840576172\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.89980697631836\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.13678741455078\n",
      "Step 6: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.37059783935547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.300020217895508\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.127506256103516\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.984844207763672\n",
      "Step 10: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.210752487182617\n",
      "Step 11: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.811765670776367\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.343286514282227\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.620445251464844\n",
      "Step 14: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.212928771972656\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.888620376586914\n",
      "Step 16: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.519336700439453\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.83930015563965\n",
      "Step 18: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.27252197265625\n",
      "Step 19: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.068580627441406\n",
      "Step 20: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.315298080444336\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.612586975097656\n",
      "Step 22: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.256023406982422\n",
      "Step 23: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.673538208007812\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.01090621948242\n",
      "Step 25: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.450218200683594\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.14035415649414\n",
      "Step 27: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 8 with cumulated_reward -1028\n",
      "loss 23.91966438293457\n",
      "done!\n",
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.298320770263672\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.769287109375\n",
      "Step 2: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.276466369628906\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.908973693847656\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.560319900512695\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.29130744934082\n",
      "Step 6: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.16851043701172\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.390098571777344\n",
      "Step 8: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.754615783691406\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.83946418762207\n",
      "Step 10: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.349504470825195\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.292705535888672\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.43895721435547\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.135622024536133\n",
      "Step 14: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 9 with cumulated_reward -1015\n",
      "loss 8.677213668823242\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.41313934326172\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.8756046295166\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.541711807250977\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.964447021484375\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.761226654052734\n",
      "Step 5: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.316810607910156\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.282222747802734\n",
      "Step 7: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.939857482910156\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.984128952026367\n",
      "Step 9: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.14873504638672\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.02558898925781\n",
      "Step 11: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.07350158691406\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.69068908691406\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.475543975830078\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.194351196289062\n",
      "Step 15: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.371295928955078\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.72696304321289\n",
      "Step 17: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.69681167602539\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.447067260742188\n",
      "Step 19: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.72875213623047\n",
      "Step 20: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 10 with cumulated_reward -1021\n",
      "loss 47.42744445800781\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.425100326538086\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.81592559814453\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.91178894042969\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.03749084472656\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.41336441040039\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.830408096313477\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.90363121032715\n",
      "Step 7: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.091344833374023\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.495250701904297\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.72279167175293\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.545333862304688\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.410531997680664\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.29193878173828\n",
      "Step 13: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.763229370117188\n",
      "Step 14: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.707773208618164\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.120445251464844\n",
      "Step 16: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.579933166503906\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.452674865722656\n",
      "Step 18: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.793926239013672\n",
      "Step 19: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.904314041137695\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.835329055786133\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.732484817504883\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.608890533447266\n",
      "Step 23: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.1827449798584\n",
      "Step 24: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.47026824951172\n",
      "Step 25: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.887863159179688\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.73604202270508\n",
      "Step 27: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.168201446533203\n",
      "Step 28: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.81545352935791\n",
      "Step 29: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.95101547241211\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.051355361938477\n",
      "Step 31: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.716251373291016\n",
      "Step 32: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.03709602355957\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.149738311767578\n",
      "Step 34: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.5173826217651367\n",
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.29570198059082\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.42988967895508\n",
      "Step 37: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.667030334472656\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.071407318115234\n",
      "Step 39: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.70862579345703\n",
      "Step 40: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.5902156829834\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.136180877685547\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.596599578857422\n",
      "Step 43: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.669883728027344\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.386253356933594\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.91250228881836\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.467323303222656\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.07172966003418\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.929168701171875\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.858287811279297\n",
      "Step 50: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.040605545043945\n",
      "Step 51: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.1346435546875\n",
      "Step 52: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.815710067749023\n",
      "Step 53: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.919639587402344\n",
      "Step 54: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.62578201293945\n",
      "Step 55: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.1995849609375\n",
      "Step 56: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.654903411865234\n",
      "Step 57: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.11192321777344\n",
      "Step 58: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.59466552734375\n",
      "Step 59: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.9930305480957\n",
      "Step 60: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.41360092163086\n",
      "Step 61: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.434414863586426\n",
      "Step 62: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.798166275024414\n",
      "Step 63: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.494998931884766\n",
      "Step 64: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.11817932128906\n",
      "Step 65: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.984336853027344\n",
      "Step 66: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.5128107666969299\n",
      "Step 67: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.989341735839844\n",
      "Step 68: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.89833068847656\n",
      "Step 69: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 11 with cumulated_reward 930\n",
      "loss 8.456974983215332\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.256370544433594\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.137883186340332\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.39222717285156\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.244266510009766\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.873125076293945\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.527694702148438\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.844186782836914\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.368258476257324\n",
      "Step 8: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.055362701416016\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.56973648071289\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.4753532409668\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.38323974609375\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.46155548095703\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.660663604736328\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.817577362060547\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.75249671936035\n",
      "Step 16: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.6854248046875\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.99700927734375\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.589906692504883\n",
      "Step 19: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.725522994995117\n",
      "Step 20: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.813108444213867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.39434051513672\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.115652084350586\n",
      "Step 23: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.66499328613281\n",
      "Step 24: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 63.344871520996094\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.113483428955078\n",
      "Step 26: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.663230895996094\n",
      "Step 27: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.4349365234375\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.552040100097656\n",
      "Step 29: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.78714942932129\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.848529815673828\n",
      "Step 31: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.812015533447266\n",
      "Step 32: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.543359756469727\n",
      "Step 33: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.414779663085938\n",
      "Step 34: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.79502487182617\n",
      "Step 35: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.250465393066406\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.5144152045249939\n",
      "Step 37: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.18545913696289\n",
      "Step 38: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.081462860107422\n",
      "Step 39: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.075618743896484\n",
      "Step 40: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.601329803466797\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.941595077514648\n",
      "Step 42: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.049907684326172\n",
      "Step 43: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.022998809814453\n",
      "Step 44: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.824668884277344\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.938114166259766\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.297706604003906\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.920754432678223\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.0380916595459\n",
      "Step 49: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.503822326660156\n",
      "Step 50: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.648462295532227\n",
      "Step 51: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.60104751586914\n",
      "Step 52: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.52106857299805\n",
      "Step 53: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.916046142578125\n",
      "Step 54: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.212055206298828\n",
      "Step 55: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.83679962158203\n",
      "Step 56: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.155765533447266\n",
      "Step 57: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.40616798400879\n",
      "Step 58: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 12 with cumulated_reward 941\n",
      "loss 31.50596809387207\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.986553192138672\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.547264099121094\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.431476593017578\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.760796546936035\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.884979248046875\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.8260383605957\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.888914108276367\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.19607925415039\n",
      "Step 8: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.443981170654297\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.766624450683594\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.82671356201172\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.381793975830078\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.07265853881836\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.601774215698242\n",
      "Step 14: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.143768310546875\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.365970611572266\n",
      "Step 16: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.96907424926758\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.371971130371094\n",
      "Step 18: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.122825622558594\n",
      "Step 19: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.124780654907227\n",
      "Step 20: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.257753372192383\n",
      "Step 21: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.4340877532959\n",
      "Step 22: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.758251190185547\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.275724411010742\n",
      "Step 24: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.000568389892578\n",
      "Step 25: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.564476013183594\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.82347869873047\n",
      "Step 27: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.668623924255371\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.070884704589844\n",
      "Step 29: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.06035614013672\n",
      "Step 30: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.59674835205078\n",
      "Step 31: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.088500022888184\n",
      "Step 32: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.5794734954834\n",
      "Step 33: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.49285125732422\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.159515380859375\n",
      "Step 35: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.827852249145508\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.81043243408203\n",
      "Step 37: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.09488868713379\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.140684127807617\n",
      "Step 39: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.869224548339844\n",
      "Step 40: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.51449203491211\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.401460647583008\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.5892333984375\n",
      "Step 43: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.33584976196289\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.904112815856934\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.95991516113281\n",
      "Step 46: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.936408996582031\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.31501579284668\n",
      "Step 48: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.98448944091797\n",
      "Step 49: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.41685676574707\n",
      "Step 50: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.534170150756836\n",
      "Step 51: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 13 with cumulated_reward 948\n",
      "loss 23.18862533569336\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.990482330322266\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.37038803100586\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.68739318847656\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.241981506347656\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.6404271721839905\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.398468017578125\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.641584396362305\n",
      "Step 7: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.07900619506836\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.5268611907959\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.30206298828125\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.001909255981445\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.456329345703125\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.807869911193848\n",
      "Step 13: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.878955841064453\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.579139709472656\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.553342819213867\n",
      "Step 16: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.16596984863281\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.37342071533203\n",
      "Step 18: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.584047317504883\n",
      "Step 19: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.688854217529297\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.289630889892578\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 14 with cumulated_reward -1022\n",
      "loss 31.382686614990234\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.35809326171875\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 22.68295669555664\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.096298217773438\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.677127838134766\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.7979736328125\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.06061363220215\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.80568504333496\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.269119262695312\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.48575973510742\n",
      "Step 9: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.468875885009766\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.733816146850586\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.903169631958008\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.770069122314453\n",
      "Step 13: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.84143829345703\n",
      "Step 14: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.61898136138916\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.15707015991211\n",
      "Step 16: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.68621826171875\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.53548240661621\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.478713989257812\n",
      "Step 19: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.892717361450195\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.58509063720703\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.4814453125\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.62877655029297\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.745136260986328\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.788423538208008\n",
      "Step 25: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.263203620910645\n",
      "Step 26: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.79419708251953\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.21923065185547\n",
      "Step 28: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.75298309326172\n",
      "Step 29: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.286975860595703\n",
      "Step 30: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.5126838684082\n",
      "Step 31: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.831451416015625\n",
      "Step 32: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 22.580780029296875\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.188899993896484\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.215490341186523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 21.97701644897461\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.97949981689453\n",
      "Step 37: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.523962020874023\n",
      "Step 38: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.031461715698242\n",
      "Step 39: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.372920989990234\n",
      "Step 40: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.88108253479004\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.234546661376953\n",
      "Step 42: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.890514373779297\n",
      "Step 43: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.1309928894043\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.959592819213867\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.67099380493164\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 37.64911651611328\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.31210708618164\n",
      "Step 48: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.84463882446289\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.587268829345703\n",
      "Step 50: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.246621131896973\n",
      "Step 51: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.527923583984375\n",
      "Step 52: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 29.601444244384766\n",
      "Step 53: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 13.154345512390137\n",
      "Step 54: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.58197021484375\n",
      "Step 55: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.910603523254395\n",
      "Step 56: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.563093185424805\n",
      "Step 57: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.44769102334976196\n",
      "Step 58: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 15 with cumulated_reward 941\n",
      "loss 16.70688247680664\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 21.0860652923584\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.18290901184082\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.65111541748047\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.498723030090332\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.16344451904297\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 13.75059700012207\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 20.520736694335938\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 28.802993774414062\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.430883407592773\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.6676558256149292\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 20.421817779541016\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.21755027770996\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.956748962402344\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 16 with cumulated_reward -1014\n",
      "loss 16.50264549255371\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.921299934387207\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.191213607788086\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 36.44929504394531\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.486221313476562\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.75664710998535\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.087825775146484\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 20.847763061523438\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.57599639892578\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.49165403842926025\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.997013092041016\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.334136962890625\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.048206329345703\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 12.524223327636719\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 27.700754165649414\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.524479866027832\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 35.65169906616211\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.686077117919922\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 27.69955825805664\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.218412399291992\n",
      "Step 19: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 50.4929084777832\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.86322021484375\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.119842529296875\n",
      "Step 22: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.553102493286133\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.774751663208008\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.364908218383789\n",
      "Step 25: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 12.123244285583496\n",
      "Step 26: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.270950317382812\n",
      "Step 27: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.331592559814453\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 11.518704414367676\n",
      "Step 29: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.08465576171875\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.82212448120117\n",
      "Step 31: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 41.792823791503906\n",
      "Step 32: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.35019874572754\n",
      "Step 33: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 17 with cumulated_reward -1034\n",
      "loss 39.34343338012695\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.7675895690918\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 49.422279357910156\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.430198669433594\n",
      "Step 3: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.677345275878906\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.02764320373535\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 49.217838287353516\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.6226478815078735\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.340229034423828\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.35645294189453\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.948328018188477\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 18 with cumulated_reward -1011\n",
      "loss 31.916423797607422\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.394240379333496\n",
      "Step 1: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.119417190551758\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.102577209472656\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.1397705078125\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.150432586669922\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.83444595336914\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.193817138671875\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.188032150268555\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 19.2640380859375\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.735504150390625\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.752880096435547\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.321537017822266\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 10.801475524902344\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.314152717590332\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.831741333007812\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.93935012817383\n",
      "Step 16: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.282167434692383\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.777510643005371\n",
      "Step 18: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.78589630126953\n",
      "Step 19: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.999656677246094\n",
      "Step 20: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.79140853881836\n",
      "Step 21: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.952022552490234\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.334156036376953\n",
      "Step 23: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 26.25775146484375\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.047168731689453\n",
      "Step 25: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.391529083251953\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.00082778930664\n",
      "Step 27: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.2735710144043\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 19 with cumulated_reward -1029\n",
      "loss 31.611982345581055\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.835115432739258\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.42970085144043\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.048267364501953\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 41.77134704589844\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.9025239944458\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 18.227130889892578\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.77782440185547\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.885041236877441\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.326309204101562\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 7.994740009307861\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 20 with cumulated_reward -1011\n",
      "loss 56.07024002075195\n",
      "done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.53669738769531\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.083236694335938\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.231346130371094\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.713634490966797\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.549095153808594\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.986454010009766\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.43150329589844\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.452091217041016\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.71844482421875\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.312259674072266\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.803455352783203\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.96700668334961\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.612075805664062\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 21 with cumulated_reward -1014\n",
      "loss 47.12397766113281\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.667999267578125\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.436891555786133\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.771347045898438\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.89943313598633\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.717065811157227\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.96995735168457\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.49711799621582\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.34440231323242\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.930723190307617\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.522323608398438\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.462503433227539\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.46540069580078\n",
      "Step 12: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.731897354125977\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.74256134033203\n",
      "Step 14: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.052074432373047\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.788211822509766\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.404563903808594\n",
      "Step 17: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.3702278137207\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.6816397309303284\n",
      "Step 19: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.0244083404541\n",
      "Step 20: action=tensor([[3]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 22 with cumulated_reward -1021\n",
      "loss 16.03181266784668\n",
      "done!\n",
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.45526123046875\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.252723693847656\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.756500244140625\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.019765853881836\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.62439727783203\n",
      "Step 5: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.96640396118164\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.02355194091797\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.0005989074707\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.876869201660156\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.6176931858062744\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 23 with cumulated_reward -1011\n",
      "loss 23.817075729370117\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.848209381103516\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.903995513916016\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.93610382080078\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.42518424987793\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.544204711914062\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.13749313354492\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.743778228759766\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.144981384277344\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.489253997802734\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.06393051147461\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 24 with cumulated_reward -1011\n",
      "loss 23.72132682800293\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 41.54421615600586\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.321340560913086\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.389485359191895\n",
      "Step 3: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 63.35859680175781\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.38823699951172\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.450611114501953\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.927146911621094\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.261987686157227\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.250064849853516\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.81553840637207\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.536487579345703\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.583003997802734\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.52550220489502\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.057920455932617\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.372369766235352\n",
      "Step 15: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.259883880615234\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.18369483947754\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 25 with cumulated_reward -1018\n",
      "loss 23.582969665527344\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.22764778137207\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.449195861816406\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.69263458251953\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.532352447509766\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.980918884277344\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.768892288208008\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.339195251464844\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.156414031982422\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.059555053710938\n",
      "Step 9: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.301284790039062\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.94989776611328\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.875603675842285\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.86018180847168\n",
      "Step 13: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.464189529418945\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.087825775146484\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.767999649047852\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.18624496459961\n",
      "Step 17: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.47107219696045\n",
      "Step 18: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.852636337280273\n",
      "Step 19: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.95208740234375\n",
      "Step 20: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.797542572021484\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.602008819580078\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.882373809814453\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.906105041503906\n",
      "Step 24: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.473499298095703\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.837181091308594\n",
      "Step 26: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.505382537841797\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.670398712158203\n",
      "Step 28: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.03802490234375\n",
      "Step 29: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.971717834472656\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.820009231567383\n",
      "Step 31: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.958521842956543\n",
      "Step 32: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.573970794677734\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.688735961914062\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.67025375366211\n",
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.73685836791992\n",
      "Step 36: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.36652374267578\n",
      "Step 37: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.770292282104492\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.771387100219727\n",
      "Step 39: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.55026626586914\n",
      "Step 40: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 63.053131103515625\n",
      "Step 41: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.76556968688965\n",
      "Step 42: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.338998794555664\n",
      "Step 43: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.845149993896484\n",
      "Step 44: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.42438507080078\n",
      "Step 45: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.35732650756836\n",
      "Step 46: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.628881454467773\n",
      "Step 47: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.078845977783203\n",
      "Step 48: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.77898597717285\n",
      "Step 49: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.181132316589355\n",
      "Step 50: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.037403106689453\n",
      "Step 51: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 56.31418991088867\n",
      "Step 52: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.90542984008789\n",
      "Step 53: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.04819869995117\n",
      "Step 54: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.90445327758789\n",
      "Step 55: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.696500778198242\n",
      "Step 56: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.957916259765625\n",
      "Step 57: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.620899200439453\n",
      "Step 58: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.2735595703125\n",
      "Step 59: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.4102897644043\n",
      "Step 60: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.646852493286133\n",
      "Step 61: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.5365804433822632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.79878807067871\n",
      "Step 63: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.93617820739746\n",
      "Step 64: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.11769104003906\n",
      "Step 65: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.26475524902344\n",
      "Step 66: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.693084716796875\n",
      "Step 67: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.136287689208984\n",
      "Step 68: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.079547882080078\n",
      "Step 69: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.562580108642578\n",
      "Step 70: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.4587459564209\n",
      "Step 71: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.420166015625\n",
      "Step 72: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.89884567260742\n",
      "Step 73: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.651256561279297\n",
      "Step 74: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.04652786254883\n",
      "Step 75: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.884187698364258\n",
      "Step 76: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.11884689331055\n",
      "Step 77: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.278873443603516\n",
      "Step 78: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.82673454284668\n",
      "Step 79: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.47649383544922\n",
      "Step 80: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.853851318359375\n",
      "Step 81: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.54983901977539\n",
      "Step 82: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.836572647094727\n",
      "Step 83: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.44917297363281\n",
      "Step 84: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.599037170410156\n",
      "Step 85: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.272525787353516\n",
      "Step 86: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.49126434326172\n",
      "Step 87: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.70613670349121\n",
      "Step 88: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.265941619873047\n",
      "Step 89: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.135174751281738\n",
      "Step 90: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.329681396484375\n",
      "Step 91: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.203983306884766\n",
      "Step 92: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.09812927246094\n",
      "Step 93: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.50902557373047\n",
      "Step 94: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.347375869750977\n",
      "Step 95: action=tensor([[0]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 26 with cumulated_reward 904\n",
      "loss 38.9100456237793\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 20.431032180786133\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.94293975830078\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.318660736083984\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.986759185791016\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.8094596862793\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.52953338623047\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.818880081176758\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 70.18760681152344\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.636455535888672\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 27 with cumulated_reward -1010\n",
      "loss 32.35502243041992\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.91100311279297\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.335410118103027\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.204498291015625\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.0451774597168\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.357819557189941\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.5485954284668\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.15018653869629\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.102752685546875\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.489973068237305\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.014158248901367\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.60323715209961\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.73057556152344\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.87604522705078\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.857418060302734\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.154512405395508\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.54151153564453\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.70162582397461\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.264394760131836\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.31899642944336\n",
      "Step 19: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.2867546081543\n",
      "Step 20: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.362621307373047\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.186065673828125\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.018165588378906\n",
      "Step 23: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.101139068603516\n",
      "Step 24: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.19298553466797\n",
      "Step 25: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.4304256439209\n",
      "Step 26: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.98594665527344\n",
      "Step 27: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.703542709350586\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.88626480102539\n",
      "Step 29: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.97709655761719\n",
      "Step 30: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.940326690673828\n",
      "Step 31: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.989622116088867\n",
      "Step 32: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.412300109863281\n",
      "Step 33: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.05982971191406\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.930614471435547\n",
      "Step 35: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.4807014465332\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.21211814880371\n",
      "Step 37: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.867609024047852\n",
      "Step 38: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.896984100341797\n",
      "Step 39: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.94851303100586\n",
      "Step 40: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.719968795776367\n",
      "Step 41: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.807498931884766\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.50578308105469\n",
      "Step 43: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.785614013671875\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.169084548950195\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.99309730529785\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.56419372558594\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.444530487060547\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.32447052001953\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.797922134399414\n",
      "Step 50: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 1.6040167808532715\n",
      "Step 51: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.321897506713867\n",
      "Step 52: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.8714599609375\n",
      "Step 53: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.944469451904297\n",
      "Step 54: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.888508796691895\n",
      "Step 55: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.630157470703125\n",
      "Step 56: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 28 with cumulated_reward 943\n",
      "loss 42.892818450927734\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.270305633544922\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.038644790649414\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.40267562866211\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.46290397644043\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.45766830444336\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.486656188964844\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.85896682739258\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.968177795410156\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.63580322265625\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.070497512817383\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.742603302001953\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.63487243652344\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.92374038696289\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.198617935180664\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.897351861000061\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.64710998535156\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.89788055419922\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.10101318359375\n",
      "Step 18: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.925521850585938\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.777095794677734\n",
      "Step 20: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.262975692749023\n",
      "Step 21: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.25111770629883\n",
      "Step 22: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.17586898803711\n",
      "Step 23: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.989013671875\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.585933685302734\n",
      "Step 25: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.735828399658203\n",
      "Step 26: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.926727294921875\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.828207969665527\n",
      "Step 28: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.08902359008789\n",
      "Step 29: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.4190616607666\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.432100296020508\n",
      "Step 31: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.34539222717285\n",
      "Step 32: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.864662170410156\n",
      "Step 33: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.660053253173828\n",
      "Step 34: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.354409217834473\n",
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.956206321716309\n",
      "Step 36: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.79319190979004\n",
      "Step 37: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.419864654541016\n",
      "Step 38: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.28394317626953\n",
      "Step 39: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.675588607788086\n",
      "Step 40: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.259057998657227\n",
      "Step 41: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.704265594482422\n",
      "Step 42: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.001588821411133\n",
      "Step 43: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.376665115356445\n",
      "Step 44: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.90211868286133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.444595336914062\n",
      "Step 46: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.838590621948242\n",
      "Step 47: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.49744415283203\n",
      "Step 48: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.440204620361328\n",
      "Step 49: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.50716781616211\n",
      "Step 50: action=tensor([[3]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 29 with cumulated_reward 949\n",
      "loss 16.77618980407715\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.619964599609375\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.679100036621094\n",
      "Step 2: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.513479232788086\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.991881370544434\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 12.766368865966797\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.985397338867188\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.106449127197266\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.88328742980957\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.871784210205078\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.546539306640625\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.914125442504883\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 18.485828399658203\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 33.674102783203125\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.861661911010742\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.212461471557617\n",
      "Step 15: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.717018127441406\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.631038665771484\n",
      "Step 17: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 49.20809555053711\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.73727035522461\n",
      "Step 19: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.860170364379883\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.21178436279297\n",
      "Step 21: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.29922103881836\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.677011489868164\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.9621655344963074\n",
      "Step 24: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.153099060058594\n",
      "Step 25: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.030216217041016\n",
      "Step 26: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.57297134399414\n",
      "Step 27: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.084259033203125\n",
      "Step 28: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.22369384765625\n",
      "Step 29: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.1031494140625\n",
      "Step 30: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.31200408935547\n",
      "Step 31: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.397991180419922\n",
      "Step 32: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.378116607666016\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.017242431640625\n",
      "Step 34: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.63163757324219\n",
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.67167663574219\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.654123306274414\n",
      "Step 37: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 78.81263732910156\n",
      "Step 38: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.492645263671875\n",
      "Step 39: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.26090431213379\n",
      "Step 40: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.848971366882324\n",
      "Step 41: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.638530731201172\n",
      "Step 42: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.73672103881836\n",
      "Step 43: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.442313194274902\n",
      "Step 44: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.819705963134766\n",
      "Step 45: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.854393005371094\n",
      "Step 46: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.781198501586914\n",
      "Step 47: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.216991424560547\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.383949279785156\n",
      "Step 49: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.31269836425781\n",
      "Step 50: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.19028663635254\n",
      "Step 51: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.63137435913086\n",
      "Step 52: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.766536712646484\n",
      "Step 53: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.86355209350586\n",
      "Step 54: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.371196746826172\n",
      "Step 55: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.35091209411621\n",
      "Step 56: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.737932205200195\n",
      "Step 57: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.28598403930664\n",
      "Step 58: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.802701950073242\n",
      "Step 59: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.90391540527344\n",
      "Step 60: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.384765625\n",
      "Step 61: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.491493225097656\n",
      "Step 62: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.54195785522461\n",
      "Step 63: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.846168518066406\n",
      "Step 64: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.337608337402344\n",
      "Step 65: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.485687255859375\n",
      "Step 66: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.459186553955078\n",
      "Step 67: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.60054016113281\n",
      "Step 68: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.26496505737305\n",
      "Step 69: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.783729553222656\n",
      "Step 70: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 30 with cumulated_reward 929\n",
      "loss 0.6789461374282837\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.580718994140625\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.884004592895508\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.627166748046875\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.89710998535156\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.045166015625\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 59.88292694091797\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.585702896118164\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 10.166505813598633\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.34368896484375\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.67078399658203\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.586700439453125\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.64301300048828\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.535058975219727\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.580244064331055\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 31 with cumulated_reward -1015\n",
      "loss 32.03644561767578\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.79018020629883\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.35528564453125\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.71829605102539\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.03883934020996\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.47734832763672\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.68317413330078\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.528274536132812\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.0311279296875\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.68416976928711\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.682478904724121\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.711021423339844\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.03685760498047\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.193862915039062\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.419363021850586\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.803525924682617\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.08251190185547\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.772886276245117\n",
      "Step 17: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.322952270507812\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 32 with cumulated_reward -1019\n",
      "loss 16.35378074645996\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.638075828552246\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.431589126586914\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.67676830291748\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.402681350708008\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.777758598327637\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.13791275024414\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.287739753723145\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.471519470214844\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.661542892456055\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.99052429199219\n",
      "Step 10: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 1.2820723056793213\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.808990478515625\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 33 with cumulated_reward -1013\n",
      "loss 32.32106399536133\n",
      "done!\n",
      "Step 0: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.1124153137207\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.474979400634766\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.88167953491211\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.234609603881836\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.02669906616211\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.07075500488281\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.824710845947266\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.797086715698242\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.88136291503906\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.496402740478516\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.59821319580078\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.00746536254883\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.59768295288086\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 48.129356384277344\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.41877746582031\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 56.212730407714844\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.327392578125\n",
      "Step 17: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.007516860961914\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 34 with cumulated_reward -1019\n",
      "loss 31.93485450744629\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.881744384765625\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.417335510253906\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 23.914142608642578\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 19.16499900817871\n",
      "Step 4: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.02125549316406\n",
      "Step 5: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.843799591064453\n",
      "Step 6: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.421884536743164\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.432138442993164\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.070335388183594\n",
      "Step 9: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.95477867126465\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.78545379638672\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 35 with cumulated_reward -1012\n",
      "loss 31.42518424987793\n",
      "done!\n",
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.835933685302734\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.67821502685547\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.734503746032715\n",
      "Step 3: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.22211456298828\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.577285766601562\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.856876373291016\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.497352600097656\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 7.9561920166015625\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.20100975036621\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.996280670166016\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.364952087402344\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.991905212402344\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.3544921875\n",
      "Step 13: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.753900527954102\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.012229919433594\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.481834411621094\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.793224334716797\n",
      "Step 17: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 36 with cumulated_reward -1018\n",
      "loss 16.127599716186523\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.133962631225586\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.0665340423584\n",
      "Step 2: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 70.48937225341797\n",
      "Step 3: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.492786407470703\n",
      "Step 4: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.63642120361328\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.765762329101562\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.59553527832031\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.51803207397461\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.111974716186523\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.25251007080078\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.71930503845215\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 25.096134185791016\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.8136351704597473\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.959383010864258\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.1571044921875\n",
      "Step 15: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.62485122680664\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.71898651123047\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.70363426208496\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.45675277709961\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.38174819946289\n",
      "Step 20: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.368061065673828\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.69795608520508\n",
      "Step 22: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.546329498291016\n",
      "Step 23: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.497560501098633\n",
      "Step 24: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.109596252441406\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.334321975708008\n",
      "Step 26: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 63.39780807495117\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.4267463684082\n",
      "Step 28: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 37 with cumulated_reward -1029\n",
      "loss 32.80785369873047\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.849796295166016\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.03020477294922\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.547500610351562\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.64704132080078\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.045719146728516\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.276500701904297\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.89984130859375\n",
      "Step 7: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.62102508544922\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.573699951171875\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.835285186767578\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.8032112121582\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.809425354003906\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.91230010986328\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.981915473937988\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.652923583984375\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.252784729003906\n",
      "Step 16: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.64432716369629\n",
      "Step 17: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.02117156982422\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.466407775878906\n",
      "Step 19: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.85254669189453\n",
      "Step 20: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.903839111328125\n",
      "Step 21: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.882798194885254\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.361228942871094\n",
      "Step 23: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.27382278442383\n",
      "Step 24: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.9827823638916\n",
      "Step 25: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.973514556884766\n",
      "Step 26: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 56.06982421875\n",
      "Step 27: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.73757553100586\n",
      "Step 28: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.860118865966797\n",
      "Step 29: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.860153198242188\n",
      "Step 30: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.311546325683594\n",
      "Step 31: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.566696166992188\n",
      "Step 32: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.90530014038086\n",
      "Step 33: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.939441680908203\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.27483081817627\n",
      "Step 35: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.981544494628906\n",
      "Step 36: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.92093276977539\n",
      "Step 37: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.85296630859375\n",
      "Step 38: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.360450744628906\n",
      "Step 39: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.62027359008789\n",
      "Step 40: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.647621154785156\n",
      "Step 41: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.23387908935547\n",
      "Step 42: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.533710479736328\n",
      "Step 43: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.432798385620117\n",
      "Step 44: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.749454498291016\n",
      "Step 45: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.490501403808594\n",
      "Step 46: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 17.677640914916992\n",
      "Step 47: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.58155632019043\n",
      "Step 48: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.010339736938477\n",
      "Step 49: action=tensor([[2]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 38 with cumulated_reward 950\n",
      "loss 46.959312438964844\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.756543159484863\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.61189651489258\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.87331771850586\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.58756446838379\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.15605545043945\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.02152633666992\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.420310974121094\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.744800567626953\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.56830596923828\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.220245361328125\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 39 with cumulated_reward -1011\n",
      "loss 15.719714164733887\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.182645797729492\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.313151359558105\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.15070724487305\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.912254333496094\n",
      "Step 4: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.36037826538086\n",
      "Step 5: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.230579376220703\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.23536682128906\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.23543167114258\n",
      "Step 8: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.09295654296875\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.23857307434082\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.363929748535156\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.095983505249023\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.4395637512207\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.77897834777832\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.004318237304688\n",
      "Step 15: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.913782119750977\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.214786529541016\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.619693756103516\n",
      "Step 18: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.21816635131836\n",
      "Step 19: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.95065689086914\n",
      "Step 20: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.07085609436035\n",
      "Step 21: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.712059020996094\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 40 with cumulated_reward -1023\n",
      "loss 16.073640823364258\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.405216217041016\n",
      "Step 1: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.828868865966797\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 63.25403594970703\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.98295593261719\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.247394561767578\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.775678634643555\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 61.92405700683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.479862213134766\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.553060531616211\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.53338050842285\n",
      "Step 10: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.26698112487793\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 41 with cumulated_reward -1012\n",
      "loss 8.063267707824707\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.783370971679688\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.0386962890625\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.348735809326172\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.270233154296875\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.596113204956055\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.76997375488281\n",
      "Step 6: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.06515884399414\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.85630798339844\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.111892700195312\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.769393920898438\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.00547218322754\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.18584442138672\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.979066848754883\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.426876068115234\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 42 with cumulated_reward -1015\n",
      "loss 39.28678894042969\n",
      "done!\n",
      "Step 0: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.10361862182617\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.34510040283203\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.315839767456055\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.99224853515625\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.185057640075684\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.506897926330566\n",
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 56.07554244995117\n",
      "Step 7: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.23318099975586\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.4161376953125\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.315311431884766\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 61.88471984863281\n",
      "Step 11: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.015457153320312\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.223541259765625\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 70.10533142089844\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 43 with cumulated_reward -1015\n",
      "loss 16.074033737182617\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.960193634033203\n",
      "Step 1: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.3507080078125\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.78145980834961\n",
      "Step 3: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.681026458740234\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.749130249023438\n",
      "Step 5: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.54762840270996\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.864341735839844\n",
      "Step 7: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.247814178466797\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.27503967285156\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.93741226196289\n",
      "Step 10: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.849729537963867\n",
      "Step 11: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.7866153717041\n",
      "Step 12: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.05017852783203\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.174264907836914\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.18968963623047\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 70.17101287841797\n",
      "Step 16: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.235742568969727\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.183223724365234\n",
      "Step 18: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.1809196472168\n",
      "Step 19: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.454954147338867\n",
      "Step 20: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.353452682495117\n",
      "Step 21: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.560760498046875\n",
      "Step 22: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.559810638427734\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.33671188354492\n",
      "Step 24: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.72577667236328\n",
      "Step 25: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.971343994140625\n",
      "Step 26: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.27310562133789\n",
      "Step 27: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.19567108154297\n",
      "Step 28: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.696361541748047\n",
      "Step 29: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.211326599121094\n",
      "Step 30: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.659679412841797\n",
      "Step 31: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.880386352539062\n",
      "Step 32: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.44701385498047\n",
      "Step 33: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.794755935668945\n",
      "Step 34: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.59640884399414\n",
      "Step 35: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 0.9310188889503479\n",
      "Step 36: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.25779724121094\n",
      "Step 37: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.463401794433594\n",
      "Step 38: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.192169189453125\n",
      "Step 39: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.33013153076172\n",
      "Step 40: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 56.063297271728516\n",
      "Step 41: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.420581817626953\n",
      "Step 42: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.88865661621094\n",
      "Step 43: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.40897750854492\n",
      "Step 44: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.42892074584961\n",
      "Step 45: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.02742385864258\n",
      "Step 46: action=tensor([[3]], device='cuda:0') reward=999 done=True info=success\n",
      "End of episode 44 with cumulated_reward 953\n",
      "loss 32.79957962036133\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.68302917480469\n",
      "Step 1: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.77557373046875\n",
      "Step 2: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.15273666381836\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.940673828125\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.230751991271973\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.737138748168945\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.47024154663086\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.967103958129883\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 40.80870056152344\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.456205368041992\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.359596252441406\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.747391700744629\n",
      "Step 12: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.526811599731445\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.964385986328125\n",
      "Step 14: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.576282501220703\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 62.57993698120117\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.00236892700195\n",
      "Step 17: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.906909942626953\n",
      "Step 18: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.446969985961914\n",
      "Step 19: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.86278533935547\n",
      "Step 20: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.495765686035156\n",
      "Step 21: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.48869323730469\n",
      "Step 22: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.534486770629883\n",
      "Step 23: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.49307632446289\n",
      "Step 24: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.74437141418457\n",
      "Step 25: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.84223747253418\n",
      "Step 26: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.087127685546875\n",
      "Step 27: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.199214935302734\n",
      "Step 28: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.797510147094727\n",
      "Step 29: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.458169937133789\n",
      "Step 30: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.27336120605469\n",
      "Step 31: action=tensor([[2]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 45 with cumulated_reward -1032\n",
      "loss 47.45534133911133\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.2259521484375\n",
      "Step 1: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.24073028564453\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.8339900970459\n",
      "Step 3: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.724979400634766\n",
      "Step 4: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.910091400146484\n",
      "Step 5: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.315275192260742\n",
      "Step 6: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.16242218017578\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.020404815673828\n",
      "Step 8: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.019676208496094\n",
      "Step 9: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.33572006225586\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 46 with cumulated_reward -1011\n",
      "loss 15.577473640441895\n",
      "done!\n",
      "Step 0: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 69.53410339355469\n",
      "Step 1: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.92112922668457\n",
      "Step 2: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.77811050415039\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.86431121826172\n",
      "Step 4: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.588321685791016\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.452598571777344\n",
      "Step 6: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.378164291381836\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.795037269592285\n",
      "Step 8: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.665006637573242\n",
      "Step 9: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.584123611450195\n",
      "Step 10: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.784446716308594\n",
      "Step 11: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.16636276245117\n",
      "Step 12: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 32.93238067626953\n",
      "Step 13: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 47.113197326660156\n",
      "Step 14: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 54.50726318359375\n",
      "Step 15: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 47 with cumulated_reward -1016\n",
      "loss 15.559542655944824\n",
      "done!\n",
      "Step 0: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 9.589560508728027\n",
      "Step 1: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.31410217285156\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.32167053222656\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.264463424682617\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.158730506896973\n",
      "Step 5: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.888803482055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 45.9075927734375\n",
      "Step 7: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.053753852844238\n",
      "Step 8: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.800653457641602\n",
      "Step 9: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 7.963082790374756\n",
      "Step 10: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.168968200683594\n",
      "Step 11: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.29717254638672\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.077983856201172\n",
      "Step 13: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 24.105422973632812\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 15.877126693725586\n",
      "Step 15: action=tensor([[4]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 48 with cumulated_reward -1016\n",
      "loss 8.213040351867676\n",
      "done!\n",
      "Step 0: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.560218811035156\n",
      "Step 1: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.608455657958984\n",
      "Step 2: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 30.812517166137695\n",
      "Step 3: action=tensor([[4]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.89175796508789\n",
      "Step 4: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.49263000488281\n",
      "Step 5: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 55.69327926635742\n",
      "Step 6: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 39.56972122192383\n",
      "Step 7: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.06266975402832\n",
      "Step 8: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.79653549194336\n",
      "Step 9: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 8.592671394348145\n",
      "Step 10: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.5947322845459\n",
      "Step 11: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 38.400428771972656\n",
      "Step 12: action=tensor([[0]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 46.21432113647461\n",
      "Step 13: action=tensor([[3]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 23.25432014465332\n",
      "Step 14: action=tensor([[2]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 16.83587646484375\n",
      "Step 15: action=tensor([[1]], device='cuda:0') reward=-1 done=False info={}\n",
      "loss 31.65509033203125\n",
      "Step 16: action=tensor([[1]], device='cuda:0') reward=-1001 done=True info=fail\n",
      "End of episode 49 with cumulated_reward -1017\n",
      "loss 17.51702880859375\n",
      "done!\n",
      "Completed with an average cumulated reward = -509.18\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "res = 0\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = numpy_to_torch(state)\n",
    "    cumulated_reward = 0\n",
    "    images = []\n",
    "\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        next_state = numpy_to_torch(next_state)\n",
    "        cumulated_reward += reward\n",
    "        print(\"Step {}: action={} reward={} done={} info={}\".format(t, action, reward, done, info))\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        if done:\n",
    "            #pdb.set_trace()\n",
    "            next_state = None\n",
    "            res += cumulated_reward\n",
    "            print(\"End of episode {} with cumulated_reward {}\".format(i_episode, cumulated_reward))\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            print('done!')\n",
    "            #episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Completed with an average cumulated reward = {}'.format(res/num_episodes))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few episodes of training, the trajectory is not collision free most of time. \n",
    "\n",
    "For a failure (collision) we get a reward of -1000 and end an episode.   \n",
    "For a succes  (target reached) we get a reward of 999.  \n",
    "**Average return after 100 episodes training: -509.18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
