#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type numerical
\biblio_style plain
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Motion Planning for Autonomous Driving
\end_layout

\begin_layout Author
Philippe Weingertner, Minnie Ho
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
There is a rich litterature related to Motion Planning and a very detailed
 survey is provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "7490340"
literal "false"

\end_inset

.
 Among the first 4 successful participants of DARPA Urban Challenge in 2007,
 the approaches vary but they fundamentally rely on a graph search where
 nodes correspond to a configuration state and edges correspond to elementary
 motion primitives.
 The runtime and state space can grow exponentially large.
 In this context, the use of efficient heuristic is important.
\end_layout

\begin_layout Standard
More recently, Reinforcement Learning and Deep RL have been investigated
 in the context of Autonomous Driving for Decision Making either at the
 Behavioural Planning or Motion Planning level.
 In papers from Volvo 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1803-10056"
literal "false"

\end_inset

 and BMW 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

, a DQN RL agent is trained to take decision at a tactical level: selecting
 maneuvers.
 But the problem with Reinforcement Learning is that a utility is optimized
 in expectation.
 So even if this utility is designed to avoid collisions, this will be optimized
 in expectation only.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

 an additional safety check layer is added after the DQN agent to eventually
 override the DQN agent decision.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1904-07189"
literal "false"

\end_inset

 RL is applied in the local planner: the action space is a set of longitudinal
 accelerations applied along a given path at a T-intersection.
 The agent is constrained to choose among a restricted set of safe actions
 per state.
 So the safety is enforce before Deep RL.
 Ultimately we may want combine both types of safety checks before and after
 an RL agent.
\end_layout

\begin_layout Standard
In gaming, AlphaGo Zero 
\begin_inset CommandInset citation
LatexCommand cite
key "Silver2017MasteringTG"
literal "false"

\end_inset

 has defeated human world champions: thanks to a combination of MCTS tree
 search and learning with Deep RL.
 A neural network biases the sampling towards the most relevant parts of
 the search tree: a learnt policy-value function is used as a heuristic
 during inference.
 There are a few differences for Motion Planning.
 The state space is continuous, not discrete, and only partially observable.
 Also self-play can not be used.
 These challenges have been recently tackled in different publications.
 The applicability of AlphaGo Zero ideas to Autonomous Driving has been
 studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1905-12197,8814125"
literal "false"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "8814125"
literal "false"

\end_inset

 the Motion Planning problem is addressed in a 2 steps path-velocity decompositi
on.
 The path planner employs hybrid A* to propose paths that are driveable
 and collision free wrt static obstacles.
 In a second step a velocity profile is generated by issuing acceleration
 commands.
 The problem is formulated as a POMDP model and solved with an online DESPOT
 solver.
 DESPOT is a sampling based tree search algorithm like MCTS which uses additiona
l lower bounds and upper bounds values.
 To guide the tree search of DESPOT, a NavA3C neural network is used.
 The NavA3C network is trained in simulation and is expected to provide
 tighter bounds.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-02680"
literal "false"

\end_inset

 the problem is considered at the behavioral planning level.
 A set of lane changes decisions are taken to navigate a highway and to
 reach an exit.
 The problem is formulated as a MDP problem.
 MCTS tree search is used as an online MDP solver and a learned policy-value
 network is used to efficiently guide the search.
 
\end_layout

\begin_layout Standard
We consider the problem of the local planner and velocity profile generation,
 similar to the first paper, but with an approach mainly aligned with the
 later one.
\end_layout

\begin_layout Section
Test Setup
\end_layout

\begin_layout Standard
The problem statement is as follows.
 Given an ego vehicle (E) with a given path of 
\begin_inset Formula $(x,y)$
\end_inset

 coordinates, find a set of acceleration decisions 
\begin_inset Formula $(a_{x},a_{y})$
\end_inset

 at discrete time steps to enable E to avoid a set of intersecting vehicles
 
\begin_inset Formula $\left\{ V\right\} .$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/ActV0.png
	lyxscale 25
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Custom openai gym Act-v1 env
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The ego car in blue has to avoid 10 intersecting vehicles to reach a goal
 point.
 The position and speed of intersecting vehicles is not known precisely:
 the ground truth is represented via dots while the position reported by
 the sensors is represented by crosses.
 A Time To Collision based on ground truth information is displayed and
 if there exists an intersecting car with a computed TTC below 10 seconds
 it is displayed in red.
 We developped our own test framework but made it compatible with 
\begin_inset CommandInset href
LatexCommand href
name "standard openai gym interfaces"
target "http://gym.openai.com/"
literal "false"

\end_inset

.
 Our simulation and test environment can be downloaded and installed from
 
\begin_inset CommandInset href
LatexCommand href
name "gym-act"
target "https://github.com/PhilippeW83440/CS221_Project/tree/master/gym-act"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Approach
\end_layout

\begin_layout Standard
We are dealing with a local planning task where a path to follow has been
 derived by some other algorithms (a variational technique or some A* or
 D* tree search algorithm).
 We have to derive a set of acceleration commands, such that we avoid dynamical
 obstacles that may cross our path.
 We handle a sequential decision making problem under uncertainty: the sensors
 provide noisy estimates.
 We first use a model based approach (an online MCTS tree search algorithm).
 We benchmark it against a model free approach (Q-learning or DQN).
 We then combine the 2 approaches, to guide the MCTS tree search with a
 learned heuristic.
 We target a fast, safe and explainable solution as required for Autonomous
 Driving.
 We expect the safety and explainability to be obtained thanks to the model
 based approach and to gain some speed-up by using a model free learned
 heuristic.
\end_layout

\begin_layout Subsection
MDP model
\end_layout

\begin_layout Standard
A representation of the states, position and speed information, in absolute
 coordinates would be 
\begin_inset Formula $S_{t}=\left\{ \left(x,y,v_{x},v_{y}\right)_{\text{ego}},\left(x,y,v_{x},v_{y}\right)_{\text{obj}_{1..10}}\right\} $
\end_inset

.
 But we use a relative and normalized representation for easier generalization
 and learning.
 In our setting the ego car drives along the y-axis only
\series bold
.
 
\end_layout

\begin_layout Itemize

\series bold
States:
\series default
 
\begin_inset Formula $S=\left\{ \left(\frac{y}{y^{max}},\frac{v_{y}}{v_{y}^{max}}\right)_{\text{ego}},\left(\frac{\Delta x}{\Delta x^{max}},\frac{\Delta y}{\Delta y^{max}},\frac{\Delta v_{x}}{\Delta v_{x}^{max}},\frac{\Delta v_{y}}{\Delta v_{y}^{max}}\right)_{\text{obj}_{1..10}}\right\} $
\end_inset

 which is a vector 
\begin_inset Formula $\in\mathbb{R}^{42}$
\end_inset

 
\end_layout

\begin_layout Standard
While the state space is continuous we use a discrete action space.
 
\end_layout

\begin_layout Itemize

\series bold
Actions:
\series default
 
\begin_inset Formula $A=\left[-2\;ms^{-2},-1\;ms^{-2},0\;ms^{-2},1\;ms^{-2},2\;ms^{-2}\right]$
\end_inset

 corresponding to the longitudinal acceleration.

\series bold
 
\end_layout

\begin_layout Standard
The Transition model corresponds to standard linear Gaussian dynamics with:
\end_layout

\begin_layout Itemize

\series bold
Transitions:
\series default
 
\begin_inset Formula $T\left(s'\mid s,a\right)\text{ for each car}_{i}\;P\left(S_{i}^{t+1}\mid S_{i}^{t},a_{i}^{t}\right)=\mathcal{N}\left(\mu=T_{s}S_{i}^{t}+T_{a}a_{i}^{t},\Sigma\right)$
\end_inset

 
\end_layout

\begin_layout Standard
Using a Constant Velocity Model with 
\begin_inset Formula $T_{s}=\begin{bmatrix}1 & 0 & \text{dt} & 0\\
0 & 1 & 0 & \text{dt}\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},S_{i}^{t+1}=\begin{bmatrix}x\\
y\\
v_{x}\\
v_{y}
\end{bmatrix},T_{a}=\begin{bmatrix}\frac{\text{dt}^{2}}{2} & 0\\
0 & \frac{\text{dt}^{2}}{2}\\
\text{dt} & 0\\
0 & \text{dt}
\end{bmatrix},a_{i}^{t}=\begin{bmatrix}a_{x}\\
a_{y}
\end{bmatrix}$
\end_inset

.
 The reward model accounts for efficiency (we penalize every step), safety
 (we heavily pernalize collisions) and comfort (we penalize strong accelerations
 and decelerations:
\end_layout

\begin_layout Itemize

\series bold
Reward
\series default
: 
\begin_inset Formula $R(s,a)=-1-1000\times1\left[\text{d(ego,obj)}_{s}\leq10\right]-1\left[\left|a\right|=2\right]$
\end_inset


\end_layout

\begin_layout Subsection
Algo 1, MCTS tree search
\end_layout

\begin_layout Standard
The MDP is solved online with MCTS tree search.
 Solving it offline with Value Iteration is not an option as we are dealing
 with a huge state space.
 MCTS 
\begin_inset CommandInset citation
LatexCommand cite
key "Kochenderfer2015"
literal "false"

\end_inset

 is one of the most successfull sampling-based online approaches used in
 recent years.
 It is the core part of AlphaGo Zero 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1712-01815"
literal "false"

\end_inset

.
 This algorithm involves running many simulations from the current state
 while updating an estimate of the state-action value function 
\begin_inset Formula $Q(s,a)$
\end_inset

 along its path of exploration.
 Online algorithms enable to reduce the search space to reachable states
 from a current state.
 MCTS balances exploration and exploitation via a method called Upper Confidence
 Bound: during the search we execute the action that maximizes 
\begin_inset Formula $Q(s,a)+c\sqrt{\frac{\log N(s)}{N(s,a)}}$
\end_inset

 where 
\begin_inset Formula $N(s),N(s,a)$
\end_inset

 track the number of times a state and state-action pair are visited.
 
\begin_inset Formula $c$
\end_inset

 is a parameter controlling the amount of exploration in the search: it
 will encourage exploring less visited 
\begin_inset Formula $(s,a)$
\end_inset

 pairs and rely on the learned policy via 
\begin_inset Formula $Q(s,a)$
\end_inset

 estimates for pairs that are well explored.
 Once we reach a state that is not part of the explored set, we iterate
 over all possible actions from that state and expand the tree.
 After the expansion stage, a rollout is performed: it consists in running
 many random simulations till we reach some depth.
 It is a Monte Carlo estimate so the rollout policy is typically stochastic
 and does not have to be close to optimal.
 The rollout policy is different than the policy used for exploration.
 Simulations, running from the root of the tree down to a leaf node expansion,
 followed by a rollout evaluation phase, are run until some stopping criterion
 is met: a time limit or a maximum number of iterations.
 We then execute the action that maximizes 
\begin_inset Formula $Q(s,a)$
\end_inset

 at the root of the tree.
 The pseudo code of the algorithm is provided below:
\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
Function{SelectAction}{$s,d$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
Loop
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Call {Simulate}{$s,d,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return arg max$_a
\backslash
text{ }Q(s,a)$
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Simulate}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
If {$s 
\backslash
notin T$} 
\end_layout

\begin_layout Plain Layout

		
\backslash
For {$a 
\backslash
in A(s)$}
\end_layout

\begin_layout Plain Layout

			
\backslash
State $(N(s,a),Q(s,a)) 
\backslash
gets (N_0(s,a),Q_0(s,a))$
\end_layout

\begin_layout Plain Layout

		
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

		
\backslash
State $T=T 
\backslash
cup 
\backslash
{s
\backslash
}$
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return 
\backslash
Call {Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
gets 
\backslash
text{arg max}_a
\backslash
text{ }Q(s,a)+c
\backslash
sqrt{
\backslash
frac{logN(s)}{N(s,a)}}$ 
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $q 
\backslash
gets r+
\backslash
lambda$ 
\backslash
Call {Simulate}{$s,d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $N(s,a) 
\backslash
gets N(s,a)+1$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $Q(s,a) 
\backslash
gets Q(s,a)+ 
\backslash
frac{q-Q(s,a)}{N(s,a)}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $q$ 
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
sim 
\backslash
pi_0(s)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $r+
\backslash
lambda$ 
\backslash
Call {Rollout}{$s',d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
One remaining problem is that in chess or go, the state space is discrete
 and the above algorithm does not cope with continuous state space: the
 same state may never be sampled more than once from the generative model
 which will result in a shallow tree with just one layer.
 The Progressive Widening variant of MCTS 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1709-06196,Couetoux:2011:CUC:2177360.2177393"
literal "false"

\end_inset

 solves this problem by controlling the sampling of new states and the sampling
 among already existing states to enable exploration in depth and not just
 in breadth.
 
\end_layout

\begin_layout Subsection
Algo 2, Approximate Q-learning
\end_layout

\begin_layout Standard
While similarly to 
\begin_inset CommandInset citation
LatexCommand cite
key "Silver2017MasteringTG,DBLP:journals/corr/abs-1905-12197,DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1712-01815,8814125"
literal "false"

\end_inset

 we intend to use Deep Learning to learn an evaluation function that will
 be used later on as a heuristic to guide the MCTS tree search, we will
 first investigate Approximate Q-learning with a reduced set of features.
 Weuse the following features extractor.
 
\begin_inset Formula $\phi(s,a)=\begin{bmatrix}s_{6\times1} & a_{1\times1} & s_{6\times1}^{2} & a_{1\times1}^{2}\end{bmatrix}^{T}$
\end_inset

 where we take into account the state vector of the ego car (2 components)
 and the state vector of the car with the smallest TTC (4 components).
 We also take into account the acceleration command of the ego car.
 We use quadratic components as well: as it is expected that the value of
 a 
\begin_inset Formula $(s,a)$
\end_inset

 tuple will depend on distances computations.
 We focus on a reduced set of relevant features to speed up training.
 The Q-function is parametrized by a vector 
\begin_inset Formula $w$
\end_inset

 with 
\begin_inset Formula $\hat{Q}_{\text{opt}}(s,a;\mathbf{w})=\mathbf{w}\cdot\phi(s,a)$
\end_inset

.
 We have 
\begin_inset Formula $\hat{V}_{\text{opt}}(s')=\underset{a'\in\text{Actions}(s')}{\max}\hat{Q}_{\text{opt}}(s',a')$
\end_inset

 and use the objective 
\begin_inset Formula $\left(\hat{Q}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right)^{2}$
\end_inset

 which leads to the following update rule while performing Sochastic Gradient
 Descent: 
\begin_inset Formula $\mathbf{w}\leftarrow\mathbf{w}-\eta\left[\hat{Q}_{\text{opt}}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right]\phi(s,a)$
\end_inset

.
 One of the problem we may encounter is that the data in simulation is not
 iid, but highly correlated from one simulation step to the other and the
 targets will vary a lot.
 This problem is typically handled by using an experience replay buffer,
 which is possible with an off policy algorithm, and using a different fixed
 Q-network for targets evaluation, which is updated less frequently than
 the Q-function used for predictions, as described in DeepMind papers 
\begin_inset CommandInset citation
LatexCommand cite
key "article,DBLP:journals/corr/MnihKSGAWR13"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Algo 3, Deep Q-learning
\end_layout

\begin_layout Standard
We use a DQN algorithm with a replay memory buffer to ensure we are dealing
 with iid samples and a target network, updated less frequently than the
 optimisation network, to stabilize the training procedure as described
 in DeepMind papers 
\begin_inset CommandInset citation
LatexCommand cite
key "article,DBLP:journals/corr/MnihKSGAWR13"
literal "false"

\end_inset

.
 We use a Huber Loss which acts as MSE when error is small and as a mean
 absolute error when error is large: to make it more robust to outliers
 when the estimates of 
\begin_inset Formula $Q$
\end_inset

 are very noisy.
 Exploration is done with an 
\begin_inset Formula $\epsilon$
\end_inset

 -greedy policy.
 We use a batch size of 
\begin_inset Formula $128$
\end_inset

, 
\begin_inset Formula $\gamma=0.999$
\end_inset

 and Adam optimizer for the Batch Gradient Descent updates.
 Programming is done with pytorch and we leverage on this code 
\begin_inset CommandInset href
LatexCommand href
name "pytorch.org"
target "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
literal "false"

\end_inset

 as a starting point for the DQN setup.
\end_layout

\begin_layout Standard
Our Neural Network has 42 neurons as input, corresponding to all cars position
 and speed information (relative and normalized coordinates), and 5 neurons
 as output, corresponding to the 5 possible control commands.
 We use a CNN architecture similarly to 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1803-10056"
literal "false"

\end_inset

, leveraging on CNN translational invariance properties: the order of informatio
n about different cars should not matter.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/CNN.jpeg
	lyxscale 25
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Q-function as a CNN network
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Algo 4, MCTS tree search with a learned heuristic 
\end_layout

\begin_layout Standard
We plan to use our learned Q-network 
\begin_inset Formula $\hat{Q}(s,a;\mathbf{w})$
\end_inset

 via approximate Q-learning or Deep Q-learning as a heuristic for MCTS tree
 search: to expand the tree in the most promising areas and hence come up
 faster with a good solution.
 A solution is considered good as soon as it is estimated collision free.
 We are dealing with uncertainty so it is an estimate.
 We may run further MCTS tree searches up to some time limit, to find even
 better solutions: faster or more comfortable.
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Standard
The source code is available here: 
\begin_inset CommandInset href
LatexCommand href
name "CS221 Project"
target "https://github.com/PhilippeW83440/CS221_Project"
literal "false"

\end_inset

.
 The baseline (simple rule - reflex based) and oracle (assuming no uncertainty
 using UCS/A* tree search) have been implemented at the proposal stage.
 For the progress report, we have a first version of Q-learning with some
 results summarized in the appendix.
 We are working on the MCTS implementation.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "project"

\end_inset


\end_layout

\end_body
\end_document
