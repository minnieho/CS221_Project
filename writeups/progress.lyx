#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type numerical
\biblio_style plain
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Motion Planning for Autonomous Driving
\end_layout

\begin_layout Author
Philippe Weingertner, Minnie Ho
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
There is a rich litterature related to Motion Planning and a very detailed
 survey of traditional methods is provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "7490340"
literal "false"

\end_inset

.
 Among the first 4 successful participants of DARPA Urban Challenge in 2007,
 the approaches vary.
 The winner, CMU Boss vehicle used variational techniques for local trajectory
 generation in a structured environment.
 This was done in a 2 steps path-velocity decomposition.
 A first step of path planning, using variational techniques, is performed
 and for every candidate path, a combination of different velocity profiles
 (constant, linear, linear ramp, trapezoidal) is applied : the combination
 of a path and velocity profile defines a trajectory.
 In unstructured environments (parking lots) or in error recovery situations
 a lattice graph in 4-dimensional configuration space (position, orientation
 and velocity) is searched with Anytime D* algorithm to find a collision-free
 path.
 More details are provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferguson2009MotionPI,article,5980223"
literal "false"

\end_inset

.
 The vehicle from Stanford used a search strategy coined Hybrid A* that
 constructs a tree of motion primitives by recursively applying a finite
 set of maneuvers.
 The search was guided by a carefully designed heuristic.
 The vehicle arriving 3rd, Victor Tango from Virginia Tech, constructs a
 graph discretization of possible maneuvers and searches the graph with
 the A* algorithm.
 The vehicle arriving 4th, developed by MIT used a variant of RRT algorithm
 with biased sampling.
 While all these techniques differ, they fundamentally rely on a graph search
 where nodes correspond to a configuration state and edges correspond to
 elementary motion primitives.
 Although they provide solutions, the runtime and state space can grow exponenti
ally large.
 In this context, the use of heuristic to guide the search is important.
 (
\series bold
NB: uncertainty is not properly considered here in these traditional MP
 methods.
 It is like reasoning with the mean state vector provided by sensor fusion
 output and ignoring the covariance matrix !
\series default
)
\end_layout

\begin_layout Standard
More recently, Reinforcement Learning and Deep RL have been investigated
 in the context of Autonomous Driving for Decision Making either at the
 Behavioural Planning or Motion Planning level.
 In some research papers from Volvo 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1803-10056"
literal "false"

\end_inset

 and BMW 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

 , an RL agent is trained in simulation to take decision at a higher tactical
 level: the decisions relate to a maneuver selection, like lane change,
 rather than a low level acceleration command.
 DQN is used to train an agent.
 But the problem with Reinforcement Learning is that the utility is optimized
 in expectation.
 So even if the reward is designed to avoid collisions, this will be optimized
 in expectation: ultimately it is as if safety would be enforced with soft
 constraints rather than hard constraints.
 Which is of course not acceptable for a real vehicle.
 To solve this problem in 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

 an additional safety check layer is added after the DQN agent to eventually
 override the DQN agent decision if it is considered unsafe.
 Checking a decision wrt to a specific criteria is simpler than designing
 a decision making system that jointly optimizes efficiency, comfort and
 safety objectives.
 With RL applied to AD we have to account for additional safety checks.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1904-07189"
literal "false"

\end_inset

 Deep RL is applied at the local planner level: the action space is a set
 of longitudinal accelerations 
\begin_inset Formula $\left\{ -4m/s^{2},-2m/s^{2},0m/s^{2},2m/s^{2}\right\} $
\end_inset

 applied along a given path at a T-intersection.
 Safety is handled in a different way here compared to previous BMW approach:
 the agent is constrained to choose among a restricted set of safe actions
 per state.
 So the safety is enforce before Deep RL.
 Ultimately car manufacters may want to combine both types of safety checks:
 constraining the action set per state before enabling an RL agent to make
 its own decision, and checking again the final sequence of decisions proposed
 by the RL agent.
\end_layout

\begin_layout Standard
Now the interesting topic is how to best combine traditional Motion Planning
 with RL.
 What are the limitations of these techniques in isolation and how to use
 the strengths of both approaches and circumvent their weaknesses.
 Traditional motion planning relies heavily on tree search and to enable
 real time solutions good heuristics are required.
 Designing a good heuristic is hard.
 What if we could learn it ? By training an agent with model free RL we
 can potentially end up with an agent that performs pretty well most of
 the time and from time to times fails miserably in a way that is hard to
 explain.
 The main problems with model free RL are sample efficiency (we need a lot
 of data), enforcing hard constraints and explainability (how can we explain
 the decision taken by a RL agent which may become a problem for a car manufactu
rer).
 While a model based planning method has the advantages of explainability,
 do not rely on data and can deal in a more systematic way with hard constraints.
 As demonstrated in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1904-11483"
literal "false"

\end_inset

 in simple situations RL methods have no benefit over rule based methods,
 pure RL does not enable the agent to act in safer way.
 But when the situation becomes much more complex with an increasing number
 of cars and pedestrians, the benefits of Deep RL methods become clear.
\end_layout

\begin_layout Standard
In the gaming domain, chess and go, performances superior to human performances
 have been achieved with AlphaGo Zero 
\begin_inset CommandInset citation
LatexCommand cite
key "Silver2017MasteringTG"
literal "false"

\end_inset

: by combining planning with MCTS tree search and learning with RL.
 A neural network biases the sampling towards the most relevant parts of
 the search tree: a learnt policy-value function is used as a heuristic
 during inference.
 While during training, MCTS is used to improve the sample efficiency of
 RL training.
 Now there are a few major differences between a game like chess or go and
 our initial Motion Planning problem.
 In chess or go the state space is discrete and fully observable while in
 AD the state space is continuous and partially observable.
 In terms of action sets in both cases, we can deal with discrete action
 sets.
 But another challenge is that self-play can not be used in the context
 of Motion Planning.
 These challenges have been recently tackled in different publications.
 The applicability of AlphaGo Zero to Autonomous Driving has been studied
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1905-12197,8814125"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "8814125"
literal "false"

\end_inset

 the Motion Planning problem is adressed in a 2 steps path-velocity decompositio
n.
 The path planner employs hybrid A* to propose paths that are driveable
 and collision free wrt static obstacles.
 In a second step a velocity profile is generated by issuing acceleration
 commands.
 The problem is formulated as a POMDP model and solved with an online DESPOT
 solver.
 DESPOT is a sampling based tree search algorithm like MCTS which uses additiona
l lower bounds and upper bounds values.
 To guide the tree search of DESPOT, a NavA3C neural network is used.
 The NavA3C network is trained in simulation and is expected to provide
 tighter bounds than the heuristic commonly used for lower and upper bounds
 estimation.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-12197"
literal "false"

\end_inset

 the problem of pedestrians collision avoidance in dense urban traffic areas
 is considered.
 Again the problem is formulated as a POMDP and DESPOT is used as an online
 solver.
 The 2 steps path-velocity decomposition is used: Hybrid A* plans a path
 and POMDP is restricted to control only the accelarations along the path.
 Two stages are considered: an imitation stage where a Neural Network is
 trained in simulation to solve the POMDP problem.
 In the improvement stage, LeTS-Drive uses Hyp-DESPOT, a massively parallel
 belief tree search algorithm, to plan vehicle motions.
 LeTS-Drive incorporates the prior knowledge learned in the policy and value
 networks into the heuristics of HyP-DESPOT in order to search efficiently
 within the limited planning time.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-02680"
literal "false"

\end_inset

 the problem is considered at a higher level: at the behavioral planning
 level.
 It is mainly a set of lane changes decisions that are taken to navigate
 a highway or to reach a highway exit.
 The problem is formulated as a MDP problem.
 MCTS tree search is used as an online MDP solver and a learned policy-value
 network is used to efficiently guide the search.
 
\end_layout

\begin_layout Standard
In our case we consider the problem of the local planner and velocity profile
 generation, similar to the 2 first papers, but with an approach mainly
 aligned with the later one.
\end_layout

\begin_layout Section
Test Setup
\end_layout

\begin_layout Standard
The problem statement is as follows.
 Given an ego vehicle (E) with a given path of 
\begin_inset Formula $(x,y)$
\end_inset

 coordinates, find a set of acceleration decisions 
\begin_inset Formula $(a_{x},a_{y})$
\end_inset

 at discrete time steps to enable E to avoid a set of intersecting vehicles
 
\begin_inset Formula $\left\{ V\right\} .$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename img/ActV0.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Standard
The ego car in blue has to avoid 10 intersecting vehicles to reach a goal
 point.
 The position and speed of intersecting vehicles is not known precisely:
 the ground truth is represented via dots while the position reported by
 the sensors is represented by crosses.
 A Time To Collision based on ground truth information is displayed and
 if there exist an intersecting car with a predicted TTC below 10 seconds
 it is displayed in red.
 This test framework is a custom one we have developped.
 We have a version of this test framework that is compatible with open ai
 gym interfaces: so that any standard Deep RL setup, can be directly used
 with this environment.
 Typically we intend to use a DQN setup from 
\begin_inset CommandInset href
LatexCommand href
name "pytorch.org"
target "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
literal "false"

\end_inset

 initially tested on cartpole-v1 with our own Act-v1 environment.
 Our simulation and test environment can be downloaded and installed from
 
\begin_inset CommandInset href
LatexCommand href
name "gym-act"
target "https://github.com/PhilippeW83440/CS221_Project/tree/master/gym-act"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Approach
\end_layout

\begin_layout Subsection
MDP model
\end_layout

\begin_layout Itemize

\series bold
State
\series default
 (absolute coordinates): 
\begin_inset Formula $S_{t}=\left\{ S_{i}^{t}\right\} _{i=0..11}=\left\{ \left(x,y,v_{x},v_{y}\right)_{\text{ego}},\left(x,y,v_{x},v_{y}\right)_{\text{obj}_{1..10}}\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
with 
\begin_inset Formula $S_{i}^{t}=\left[x,y,v_{x},v_{y}\right]^{T}$
\end_inset

 and 
\begin_inset Formula $i\in\left[0,11\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
We use a relative and normalized 
\begin_inset Formula $\in\left[-1,1\right]$
\end_inset

 representation of the state and account for the fact that the ego car drives
 along the y-axis only (in a further simplified first version we could even
 assume that the 10 other cars drive only along the x-axis, crossing orthogonall
y to the ego car path):
\end_layout

\begin_layout Itemize

\series bold
State
\series default
 (relative coordinates): 
\begin_inset Formula $S_{t}=\left\{ S_{i}^{t}\right\} _{i=0..11}=\left\{ \left(\frac{y}{y^{max}},\frac{v_{y}}{v_{y}^{max}}\right)_{\text{ego}},\left(\frac{\Delta x}{\Delta x^{max}},\frac{\Delta y}{\Delta y^{max}},\frac{\Delta v_{x}}{\Delta v_{x}^{max}},\frac{\Delta v_{y}}{\Delta v_{y}^{max}}\right)_{\text{obj}_{1..10}}\right\} $
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Actions
\series default
: 
\begin_inset Formula $a\in\left[-2\;ms^{-2},-1\;ms^{-2},0\;ms^{-2},1\;ms^{-2},2\;ms^{-2}\right]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
for ego vehicle we choose an acceleration along y-axis
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Transitions
\series default
: 
\begin_inset Formula $T\left(s'\mid s,a\right)=P\left(S_{i}^{t+1}\mid S_{i}^{t},a\right)=\mathcal{N}\left(T_{s}S_{i}^{t}+T_{a}\begin{bmatrix}a_{x}\\
a_{y}
\end{bmatrix}\right)$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Linear Gaussian dynamics with a Constant Velocity Model
\end_layout

\begin_layout Itemize
\begin_inset Formula $T_{s}=\begin{bmatrix}1 & 0 & \text{dt} & 0\\
0 & 1 & 0 & \text{dt}\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},T_{a}=\begin{bmatrix}\frac{\text{dt}^{2}}{2} & 0\\
0 & \frac{\text{dt}^{2}}{2}\\
\text{dt} & 0\\
0 & \text{dt}
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\begin{bmatrix}x^{t+1}\\
y^{t+1}\\
v_{x}^{t+1}\\
v_{y}^{t+1}
\end{bmatrix}=\begin{bmatrix}x^{t}+v_{x}^{t}\text{dt}\\
y^{t}+v_{y}^{t}\text{dt}\\
v_{x}^{t}\\
v_{y}^{t}
\end{bmatrix}+\begin{bmatrix}a_{x}\frac{\text{dt}^{2}}{2}\\
a_{y}\frac{\text{dt}^{2}}{2}\\
a_{x}\text{dt}\\
a_{y}\text{dt}
\end{bmatrix}+\begin{bmatrix}\mathcal{\sigma}_{x}\\
\sigma_{y}\\
\sigma_{v_{x}}\\
\sigma_{v_{y}}
\end{bmatrix}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Reward
\series default
: efficiency + safety + comfort
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R_{t}=-1-1000\times1\left[\text{d(ego,obj)}\leq10\right]-1\left[\left|a_{t}\right|=2\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Algo 1, MCTS tree search
\end_layout

\begin_layout Standard
The MDP is solved online with MCTS tree search.
 Solving it offline with Value Iteration is not an option as we are dealing
 with a huge state space.
 MCTS is one of the most successfull sampling-based online approaches used
 in recent years.
 It is the core part of AlphaGo Zero 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1712-01815"
literal "false"

\end_inset

.
 A description of the algorithm is provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "Kochenderfer2015"
literal "false"

\end_inset

.
 This algorithm involves running many simulations from the current state
 while updating an estimate of the state-action value function 
\begin_inset Formula $Q(s,a)$
\end_inset

 along its path of exploration.
 Online algorithms enable to reduce the search space to the portion of the
 state space that is reachable from a current state.
 MCTS has the possibility to balance exploration and exploitation typically
 via a method called Upper Confidence Bound: during the search we execute
 the action that maximizes 
\begin_inset Formula $Q(s,a)+c\sqrt{\frac{\log N(s)}{N(s,a)}}$
\end_inset

 where 
\begin_inset Formula $N(s),N(s,a)$
\end_inset

 track the number of times a state and state-action pair are visited .
 
\begin_inset Formula $c$
\end_inset

 is a parameter that controls the amount of exploration in the search: it
 will encourage exploring less visited 
\begin_inset Formula $(s,a)$
\end_inset

 pairs and rely on the learned policy via 
\begin_inset Formula $Q(s,a)$
\end_inset

 estimates for pairs that are well expolored, to choose an action from.
 Once we reach a state that is not part of the explored set, we iterate
 over all possible actions from that state and expand the tree.
 After the expansion stage, a rollout is performed: the rollout consists
 in running many random simulations till we reach some depth.
 It is a Monte Carlo estimate of a state so the rollout policy are typically
 stochastic and do not have to be close to optimal.
 The rollout policy is different than the policy used for exploitation/explorati
on presented above.
 Simulations, running from the root of the tree down to a leaf node expansion,
 followed by a rollout evaluation phase, are run until some stopping criterion
 is met: a time limit or a maximum number of iterations.
 We then execute the action that maximizes 
\begin_inset Formula $Q(s,a)$
\end_inset

 at the root of the tree.
 The pseudo code of the algorithm is provided below:
\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
Function{SelectAction}{$s,d$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
Loop
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Call {Simulate}{$s,d,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return arg max$_a
\backslash
text{ }Q(s,a)$
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Simulate}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
If {$s 
\backslash
notin T$} 
\end_layout

\begin_layout Plain Layout

		
\backslash
For {$a 
\backslash
in A(s)$}
\end_layout

\begin_layout Plain Layout

			
\backslash
State $(N(s,a),Q(s,a)) 
\backslash
gets (N_0(s,a),Q_0(s,a))$
\end_layout

\begin_layout Plain Layout

		
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

		
\backslash
State $T=T 
\backslash
cup 
\backslash
{s
\backslash
}$
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return 
\backslash
Call {Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
gets 
\backslash
text{arg max}_a
\backslash
text{ }Q(s,a)+c
\backslash
sqrt{
\backslash
frac{logN(s)}{N(s,a)}}$ 
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $q 
\backslash
gets r+
\backslash
lambda$ 
\backslash
Call {Simulate}{$s,d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $N(s,a) 
\backslash
gets N(s,a)+1$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $Q(s,a) 
\backslash
gets Q(s,a)+ 
\backslash
frac{q-Q(s,a)}{N(s,a)}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $q$ 
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
sim 
\backslash
pi_0(s)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $r+
\backslash
lambda$ 
\backslash
Call {Rollout}{$s',d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
One remaining problem is that in chess or go, state space is discrete and
 the above algorithm does not cope with continuous state space: the same
 state may never be sampled more than once from the generative model which
 will result in a shallow tree with just one layer.
 The Progressive Widening variant of MCTS 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1709-06196,Couetoux:2011:CUC:2177360.2177393"
literal "false"

\end_inset

 solves this problem by controlling the sampling of new states and the sampling
 among already existing states to enable exploration in depth and not just
 in breadth.
 
\end_layout

\begin_layout Subsection
Algo 2, Approximate Q-learning
\end_layout

\begin_layout Standard
While ultimately and similarly to the papers referenced in the literature
 review section 
\begin_inset CommandInset citation
LatexCommand cite
key "Silver2017MasteringTG,DBLP:journals/corr/abs-1905-12197,DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1712-01815,8814125"
literal "false"

\end_inset

 we intend to use Deep Learning to learn an evaluation function that will
 be used later on as a heuristic to guide the MCTS tree search, we will
 first try Approximate Q-learning to check how it performs with a simple
 set of handcrafted features.
 We first derive a simple features extractor.
 
\begin_inset Formula $\phi(s,a)=\begin{bmatrix}s_{6\times1}\\
a_{1\times1}\\
s_{6\times1}^{2}\\
a_{1\times1}^{2}
\end{bmatrix}$
\end_inset

 where we take into account the state vector of the ego car (2 components)
 and the state vector of the car with the smallest TTC (4 components).
 We also take into account the acceleration command of the ego car.
 We use quadratic components as well: as it is expected that the value of
 a 
\begin_inset Formula $(s,a)$
\end_inset

 tuple will depend on distances computations.
 Typically when computing Time To Collision, quadratic terms appear so we
 want to provide these relevant features and figure out by learning the
 weights associated to these features.
 We try to focus on a reduced set of relevant features to speed up training.
 The Q-function is parametrized by a vector 
\begin_inset Formula $w$
\end_inset

 with 
\begin_inset Formula $\hat{Q}_{\text{opt}}(s,a;\mathbf{w})=\mathbf{w}\cdot\phi(s,a)$
\end_inset

.
 We have 
\begin_inset Formula $\hat{V}_{\text{opt}}(s')=\underset{a'\in\text{Actions}(s')}{\max}\hat{Q}_{\text{opt}}(s',a')$
\end_inset

 and use the objective 
\begin_inset Formula $\left(\hat{Q}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right)^{2}$
\end_inset

 which leads to the following update rule while performing Sochastic Gradient
 Descent: 
\begin_inset Formula $\mathbf{w}\leftarrow\mathbf{w}-\eta\left[\hat{Q}_{\text{opt}}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right]\phi(s,a)$
\end_inset

.
 One of the problem we may encounter is that the data in simulation is not
 iid, but highly correlated from one simulation step to the other and the
 targets will vary a lot.
 This problem is typically handled by using an experience replay buffer
 (which is possible with an off policy algorithm) and using a different
 fixed Q-network for targets evaluation: which is updated less frequently
 than the Q-function used for predictions: as described in DeepMind papers
 
\begin_inset CommandInset citation
LatexCommand cite
key "article,DBLP:journals/corr/MnihKSGAWR13"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Algo 3, Deep Q-learning
\end_layout

\begin_layout Standard
Model Free RL method.
 Our Neural Network has 42 neurons as input, corresponding to the state
 vector, and 5 neurons as output.
 We will use a Neural Network architecture slightly adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1803-10056"
literal "false"

\end_inset

.
 It is based on a CNN network as we want to have translational invariance
 of the input.
 It should not matter to provide information about different cars in one
 order or the other.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename img/CNN_for_DQN.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Subsection
Algo 4, MCTS tree search with a learned heuristic 
\end_layout

\begin_layout Standard
Combining Planning and Learning, Model Based and Model Free RL.
 TODO post progress report.
\end_layout

\begin_layout Standard
We want to use our learned Q-network 
\begin_inset Formula $\hat{Q}(s,a;\mathbf{w})$
\end_inset

 via approximate Q-learning or Deep Q-learning as a heuristic for MCTS tree
 search: to expand the tree in the most promising areas and hence come up
 faster with a good solution.
 A solution is considered good as soon as it is estimated collision free;
 we may run further MCTS tree searches up to some time limit, to find even
 better solutions: faster or more comfortable.
 The reward takes into account safety, comfort and efficiency.
 While there is a big penalty for collisions of 
\begin_inset Formula $-1000,$
\end_inset

 at every time step we penalize the reward by 
\begin_inset Formula $-1$
\end_inset

 to enforce efficiency and penalize every strong acceleration or deceleration,
 when 
\begin_inset Formula $\left|a\right|=2\quad ms^{-2}$
\end_inset

, by -2 to encourage more comfortable trajectories.
\end_layout

\begin_layout Section
Experimental Setup and Status
\end_layout

\begin_layout Standard
The source code is available here: 
\begin_inset CommandInset href
LatexCommand href
name "CS221 Project"
target "https://github.com/PhilippeW83440/CS221_Project"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Baseline: simple rule - reflex based.
 DONE
\end_layout

\begin_layout Itemize
Oracle: assumes no uncertainty, UCS/A* tree search.
 DONE
\end_layout

\begin_layout Itemize
Sequential Decision Making with Uncertainty => solve a MDP
\end_layout

\begin_deeper
\begin_layout Itemize
Planning with MCTS tree search.
 ON-GOING: MDP model implementation + our own MCTS algo implementation
\end_layout

\begin_layout Itemize
Approximate Q-learning: leverage on CS221 hw4 blackjack setup + custom Features
 Extractor + interface with our MDP model
\end_layout

\begin_layout Itemize
Deep Q-learning: basically the pytorch DQN setup from 
\begin_inset CommandInset href
LatexCommand href
name "pytorch.org"
target "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
literal "false"

\end_inset

 customized with our proposed CNN network and interfaced with our own gymai
 environment (gymai-act for Anti Collision Tests)
\end_layout

\begin_layout Itemize
Combining Planning and Learning: we should have MCTS and Q-learning working
 independently first ..
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "project"

\end_inset


\end_layout

\end_body
\end_document
