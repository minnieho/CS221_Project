#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type numerical
\biblio_style plain
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Motion Planning for Autonomous Driving
\end_layout

\begin_layout Author
Philippe Weingertner, Minnie Ho
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
There is a rich litterature related to Motion Planning and a very detailed
 survey of traditional methods is provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "7490340"
literal "false"

\end_inset

.
 Among the first 4 successful participants of DARPA Urban Challenge in 2007,
 the approaches vary.
 The winner, CMU Boss vehicle used variational techniques for local trajectory
 generation in a structured environment.
 This was done in a 2 steps path-velocity decomposition.
 A first step of path planning, using variational techniques, is performed
 and for every candidate path, a combination of different velocity profiles
 (constant, linear, linear ramp, trapezoidal) is applied : the combination
 of a path and velocity profile defines a trajectory.
 In unstructured environments (parking lots) or in error recovery situations
 a lattice graph in 4-dimensional configuration space (position, orientation
 and velocity) is searched with Anytime D* algorithm to find a collision-free
 path.
 More details are provided in 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferguson2009MotionPI,article,5980223"
literal "false"

\end_inset

.
 The vehicle from Stanford used a search strategy coined Hybrid A* that
 constructs a tree of motion primitives by recursively applying a finite
 set of maneuvers.
 The search was guided by a carefully designed heuristic.
 The vehicle arriving 3rd, Victor Tango from Virginia Tech, constructs a
 graph discretization of possible maneuvers and searches the graph with
 the A* algorithm.
 The vehicle arriving 4th, developed by MIT used a variant of RRT algorithm
 with biased sampling.
 While all these techniques differ, they fundamentally rely on a graph search
 where nodes correspond to a configuration state and edges correspond to
 elementary motion primitives.
 Although they provide solutions, the runtime and state space can grow exponenti
ally large.
 In this context, the use of heuristic to guide the search is important.
 (
\series bold
NB: uncertainty is not properly considered here in these traditional MP
 methods.
 It is like reasoning with the mean state vector provided by sensor fusion
 output and ignoring the covariance matrix !
\series default
)
\end_layout

\begin_layout Standard
More recently, Reinforcement Learning and Deep RL have been investigated
 in the context of Autonomous Driving for Decision Making either at the
 Behavioural Planning or Motion Planning level.
 In some research papers from Volvo 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1803-10056"
literal "false"

\end_inset

 and BMW 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

 , an RL agent is trained in simulation to take decision at a higher tactical
 level: the decisions relate to a maneuver selection, like lane change,
 rather than a low level acceleration command.
 DQN is used to train an agent.
 But the problem with Reinforcement Learning is that the utility is optimized
 in expectation.
 So even if the reward is designed to avoid collisions, this will be optimized
 in expectation: ultimately it is as if safety would be enforced with soft
 constraints rather than hard constraints.
 Which is of course not acceptable for a real vehicle.
 To solve this problem in 
\begin_inset CommandInset citation
LatexCommand cite
key "inproceedings"
literal "false"

\end_inset

 an additional safety check layer is added after the DQN agent to eventually
 override the DQN agent decision if it is considered unsafe.
 Checking a decision wrt to a specific criteria is simpler than designing
 a decision making system that jointly optimizes efficiency, comfort and
 safety objectives.
 With RL applied to AD we have to account for additional safety checks.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1904-07189"
literal "false"

\end_inset

 Deep RL is applied at the local planner level: the action space is a set
 of longitudinal accelerations 
\begin_inset Formula $\left\{ -4m/s^{2},-2m/s^{2},0m/s^{2},2m/s^{2}\right\} $
\end_inset

 applied along a given path at a T-intersection.
 Safety is handled in a different way here compared to previous BMW approach:
 the agent is constrained to choose among a restricted set of safe actions
 per state.
 So the safety is enforce before Deep RL.
 Ultimately car manufacters may want to combine both types of safety checks:
 constraining the action set per state before enabling an RL agent to make
 its own decision, and checking again the final sequence of decisions proposed
 by the RL agent.
\end_layout

\begin_layout Standard
Now the interesting topic is how to best combine traditional Motion Planning
 with RL.
 What are the limitations of these techniques in isolation and how to use
 the strengths of both approaches and circumvent their weaknesses.
 Traditional motion planning relies heavily on tree search and to enable
 real time solutions good heuristics are required.
 Designing a good heuristic is hard.
 What if we could learn it ? By training an agent with model free RL we
 can potentially end up with an agent that performs pretty well most of
 the time and from time to times fails miserably in a way that is hard to
 explain.
 The main problems with model free RL are sample efficiency (we need a lot
 of data), enforcing hard constraints and explainability (how can we explain
 the decision taken by a RL agent which may become a problem for a car manufactu
rer).
 While a model based planning method has the advantages of explainability,
 do not rely on data and can deal in a more systematic way with hard constraints.
 As demonstrated in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1904-11483"
literal "false"

\end_inset

 in simple situations RL methods have no benefit over rule based methods,
 pure RL does not enable the agent to act in safer way.
 But when the situation becomes much more complex with an increasing number
 of cars and pedestrians, the benefits of Deep RL methods become clear.
\end_layout

\begin_layout Standard
In the gaming domain, chess and go, performances superior to human performances
 have been achieved with AlphaGo Zero 
\begin_inset CommandInset citation
LatexCommand cite
key "Silver2017MasteringTG"
literal "false"

\end_inset

: by combining planning with MCTS tree search and learning with RL.
 A neural network biases the sampling towards the most relevant parts of
 the search tree: a learnt policy-value function is used as a heuristic
 during inference.
 While during training, MCTS is used to improve the sample efficiency of
 RL training.
 Now there are a few major differences between a game like chess or go and
 our initial Motion Planning problem.
 In chess or go the state space is discrete and fully observable while in
 AD the state space is continuous and partially observable.
 In terms of action sets in both cases, we can deal with discrete action
 sets.
 But another challenge is that self-play can not be used in the context
 of Motion Planning.
 These challenges have been recently tackled in different publications.
 The applicability of AlphaGo Zero to Autonomous Driving has been studied
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1905-12197,8814125"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
TODO: 
\end_layout

\begin_layout Itemize
analysis of the 3 most relevant papers
\end_layout

\begin_layout Itemize
highlight what is specific to our use case
\end_layout

\begin_layout Section
Approach
\end_layout

\begin_layout Standard
A few notes + baseline / oracle recap + explain our Custom openai-gym env
\end_layout

\begin_layout Subsection
MDP model
\end_layout

\begin_layout Standard
WARNING: use a relative representation of the state (to reduce state space
 
\begin_inset Quotes eld
\end_inset

area
\begin_inset Quotes erd
\end_inset

, enable easier generalization/learning etc ...)
\end_layout

\begin_layout Itemize

\series bold
State
\series default
: 
\begin_inset Formula $S_{t}=\left\{ S_{i}^{t}\right\} _{i=0..11}=\left\{ \left(x,y,v_{x},v_{y}\right)_{\text{ego}},\left(x,y,v_{x},v_{y}\right)_{\text{obj}_{1..10}}\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
with 
\begin_inset Formula $S_{i}^{t}=\left[x,y,v_{x},v_{y}\right]^{T}$
\end_inset

 and 
\begin_inset Formula $i\in\left[0,11\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Actions
\series default
: 
\begin_inset Formula $a\in\left[-2\;ms^{-2},-1\;ms^{-2},0\;ms^{-2},1\;ms^{-2},2\;ms^{-2}\right]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
for ego vehicle we choose an acceleration along y-axis
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Transitions
\series default
: 
\begin_inset Formula $T\left(s'\mid s,a\right)=P\left(S_{i}^{t+1}\mid S_{i}^{t},a\right)=\mathcal{N}\left(T_{s}S_{i}^{t}+T_{a}\begin{bmatrix}a_{x}\\
a_{y}
\end{bmatrix}\right)$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Linear Gaussian dynamics with a Constant Velocity Model
\end_layout

\begin_layout Itemize
\begin_inset Formula $T_{s}=\begin{bmatrix}1 & 0 & \text{dt} & 0\\
0 & 1 & 0 & \text{dt}\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},T_{a}=\begin{bmatrix}\frac{\text{dt}^{2}}{2} & 0\\
0 & \frac{\text{dt}^{2}}{2}\\
\text{dt} & 0\\
0 & \text{dt}
\end{bmatrix}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\begin{bmatrix}x^{t+1}\\
y^{t+1}\\
v_{x}^{t+1}\\
v_{y}^{t+1}
\end{bmatrix}=\begin{bmatrix}x^{t}+v_{x}^{t}\text{dt}\\
y^{t}+v_{y}^{t}\text{dt}\\
v_{x}^{t}\\
v_{y}^{t}
\end{bmatrix}+\begin{bmatrix}a_{x}\frac{\text{dt}^{2}}{2}\\
a_{y}\frac{\text{dt}^{2}}{2}\\
a_{x}\text{dt}\\
a_{y}\text{dt}
\end{bmatrix}+\begin{bmatrix}\mathcal{\sigma}_{x}\\
\sigma_{y}\\
\sigma_{v_{x}}\\
\sigma_{v_{y}}
\end{bmatrix}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Reward
\series default
: efficiency + safety + comfort
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $R_{t}=-1-1000\times1\left[\text{d(ego,obj)}\leq10\right]-1\left[\left|a_{t}\right|=2\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Algo 1, MCTS tree search
\end_layout

\begin_layout Standard
The MDP is solved online via MCTS tree search.
 This planning method, RL Model based, has a complexity that does not grow
 exponentially with the horizon.
 TODO: our own implementation.
\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
Function{SelectAction}{$s,d$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
Loop
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Call {Simulate}{$s,d,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return arg max$_a
\backslash
text{ }Q(s,a)$
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Simulate}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
If {$s 
\backslash
notin T$} 
\end_layout

\begin_layout Plain Layout

		
\backslash
For {$a 
\backslash
in A(s)$}
\end_layout

\begin_layout Plain Layout

			
\backslash
State $(N(s,a),Q(s,a)) 
\backslash
gets (N_0(s,a),Q_0(s,a))$
\end_layout

\begin_layout Plain Layout

		
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

		
\backslash
State $T=T 
\backslash
cup 
\backslash
{s
\backslash
}$
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return 
\backslash
Call {Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
gets 
\backslash
text{arg max}_a
\backslash
text{ }Q(s,a)+c
\backslash
sqrt{
\backslash
frac{logN(s)}{N(s,a)}}$ 
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $q 
\backslash
gets r+
\backslash
lambda$ 
\backslash
Call {Simulate}{$s,d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout

	
\backslash
State $N(s,a) 
\backslash
gets N(s,a)+1$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $Q(s,a) 
\backslash
gets Q(s,a)+ 
\backslash
frac{q-Q(s,a)}{N(s,a)}$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $q$ 
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
Function{Rollout}{$s,d,
\backslash
pi_0$} 
\end_layout

\begin_layout Plain Layout

	
\backslash
If {$d=0$}
\end_layout

\begin_layout Plain Layout

		
\backslash
State 
\backslash
Return $0$
\end_layout

\begin_layout Plain Layout

	
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

	
\backslash
State $a 
\backslash
sim 
\backslash
pi_0(s)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State $(s',r) 
\backslash
sim G(s,a)$
\end_layout

\begin_layout Plain Layout

	
\backslash
State 
\backslash
Return $r+
\backslash
lambda$ 
\backslash
Call {Rollout}{$s',d-1,
\backslash
pi_0$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFunction
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
TODO: add progressive widening to deal with a continuous state space.
\end_layout

\begin_layout Subsection
Algo 2, Approximate Q-learning
\end_layout

\begin_layout Standard
Model Free RL method
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{Q}_{\text{opt}}(s,a;\mathbf{w})=\mathbf{w}\cdot\phi(s,a)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{V}_{\text{opt}}(s')=\underset{a'\in\text{Actions}(s')}{\max}\hat{Q}_{\text{opt}}(s',a')$
\end_inset


\end_layout

\begin_layout Itemize

\color black
\begin_inset Formula $\text{Objective=}\left(\hat{Q}_{\text{opt}}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathbf{w}\leftarrow\mathbf{w}-\eta\left[\hat{Q}_{\text{opt}}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right]\phi(s,a)$
\end_inset

 
\end_layout

\begin_layout Subsection
Algo 3, Deep Q-learning
\end_layout

\begin_layout Standard
Model Free RL method.
 TODO post progress report.
\end_layout

\begin_layout Subsection
Algo 4, MCTS tree search with a learned heuristic 
\end_layout

\begin_layout Standard
Combining Planning and Learning, Model Based and Model Free RL.
 TODO post progress report.
\end_layout

\begin_layout Section
Experimental Setup and Status
\end_layout

\begin_layout Standard
The source code is available here: 
\begin_inset CommandInset href
LatexCommand href
name "CS221 Project"
target "https://github.com/PhilippeW83440/CS221_Project"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Baseline: simple rule - reflex based
\end_layout

\begin_layout Itemize
Oracle: assumes no uncertainty, UCS/A* tree search
\end_layout

\begin_layout Itemize
Sequential Decision Making with Uncertainty => solve a MDP
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Planning with MCTS tree search
\end_layout

\begin_layout Itemize

\series bold
Approximate Q-learning
\end_layout

\begin_layout Itemize
Deep Q-learning
\end_layout

\begin_layout Itemize
Combining Planning and Learning (with an efficient learned heuristic)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "project"

\end_inset


\end_layout

\end_body
\end_document
