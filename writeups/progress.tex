%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twocolumn,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color}
\usepackage{babel}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{algorithm,algpseudocode}

\makeatother

\usepackage[style=numeric]{biblatex}
\addbibresource{project.bib}
\begin{document}
\title{Motion Planning for Autonomous Driving}
\author{Philippe Weingertner, Minnie Ho}
\maketitle

\section{Introduction}

\section{Related Work}

There is a rich litterature related to Motion Planning and a very
detailed survey of traditional methods is provided in \cite{7490340}.
Among the first 4 successful participants of DARPA Urban Challenge
in 2007, the approaches vary but they fundamentally rely on a graph
search where nodes correspond to a configuration state and edges correspond
to elementary motion primitives. Although they provide solutions,
the runtime and state space can grow exponentially large. In this
context, the use of heuristic to guide the search is important.

More recently, Reinforcement Learning and Deep RL have been investigated
in the context of Autonomous Driving for Decision Making either at
the Behavioural Planning or Motion Planning level. In some research
papers from Volvo \cite{DBLP:journals/corr/abs-1803-10056} and BMW
\cite{inproceedings} , an RL agent is trained in simulation to take
decision at a tactical level: the decisions relate to a maneuver selection.
DQN is used to train an agent. But the problem with Reinforcement
Learning is that the utility is optimized in expectation. So even
if the reward is designed to avoid collisions, this will be optimized
in expectation only. To solve this problem in \cite{inproceedings}
an additional safety check layer is added after the DQN agent to eventually
override the DQN agent decision. In \cite{DBLP:journals/corr/abs-1904-07189}
Deep RL is applied at the local planner level: the action space is
a set of longitudinal accelerations $\left\{ -4m/s^{2},-2m/s^{2},0m/s^{2},2m/s^{2}\right\} $
applied along a given path at a T-intersection. The agent is constrained
to choose among a restricted set of safe actions per state. So the
safety is enforce before Deep RL. Ultimately we may want combine both
types of safety checks: constraining the action set per state before
enabling an RL agent to make its own decision, and checking again
the final sequence of decisions proposed by the RL agent.

In the gaming domain, performances superior to human performances
have been achieved with AlphaGo Zero \cite{Silver2017MasteringTG}:
by combining planning with MCTS tree search and learning with RL.
A neural network biases the sampling towards the most relevant parts
of the search tree: a learnt policy-value function is used as a heuristic
during inference. Now there are a few major differences between chess
or go and our Motion Planning problem. In chess or go the state space
is discrete and fully observable while in AD the state space is continuous
and partially observable. In terms of action sets in both cases, we
can deal with discrete action sets. But another difference is that
self-play can not be used in the context of Motion Planning. These
challenges have been recently tackled in different publications. The
applicability of AlphaGo Zero to Autonomous Driving has been studied
in \cite{DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1905-12197,8814125}.
In \cite{8814125} the Motion Planning problem is adressed in a 2
steps path-velocity decomposition. The path planner employs hybrid
A{*} to propose paths that are driveable and collision free wrt static
obstacles. In a second step a velocity profile is generated by issuing
acceleration commands. The problem is formulated as a POMDP model
and solved with an online DESPOT solver. DESPOT is a sampling based
tree search algorithm like MCTS which uses additional lower bounds
and upper bounds values. To guide the tree search of DESPOT, a NavA3C
neural network is used. The NavA3C network is trained in simulation
and is expected to provide tighter bounds than the heuristic commonly
used for lower and upper bounds estimation. In \cite{DBLP:journals/corr/abs-1905-02680}
the problem is considered at a higher level: at the behavioral planning
level. It is mainly a set of lane changes decisions that are taken
to navigate a highway or to reach a highway exit. The problem is formulated
as a MDP problem. MCTS tree search is used as an online MDP solver
and a learned policy-value network is used to efficiently guide the
search. 

We consider the problem of the local planner and velocity profile
generation, similar to the first paper, but with an approach mainly
aligned with the later one.

\section{Test Setup}

The problem statement is as follows. Given an ego vehicle (E) with
a given path of $(x,y)$ coordinates, find a set of acceleration decisions
$(a_{x},a_{y})$ at discrete time steps to enable E to avoid a set
of intersecting vehicles $\left\{ V\right\} .$

\includegraphics[scale=0.25]{img/ActV0}

The ego car in blue has to avoid 10 intersecting vehicles to reach
a goal point. The position and speed of intersecting vehicles is not
known precisely: the ground truth is represented via dots while the
position reported by the sensors is represented by crosses. A Time
To Collision based on ground truth information is displayed and if
there exist an intersecting car with a predicted TTC below 10 seconds
it is displayed in red. This test framework is a custom one we have
developped. We have a version of this test framework that is compatible
with open ai gym interfaces: so that any standard Deep RL setup, can
be directly used with this environment. Typically we intend to use
a DQN setup from \href{https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html}{pytorch.org}
initially tested on cartpole-v1 with our own Act-v1 environment. Our
simulation and test environment can be downloaded and installed from
\href{https://github.com/PhilippeW83440/CS221_Project/tree/master/gym-act}{gym-act}.

\section{Approach}

We are dealing with a local planning task where a path to follow has
been derived by some other algorithms (a variational technique or
some A{*} or D{*} tree search algorithm). We have to derive a set
of acceleration commands, such that we avoid dynamical obstacles that
may cross our path. We handle a sequential decision making problem
under uncertainty: the sensors provide noisy estimates. We first use
a model based approach (an online MCTS tree search algorithm). We
benchmark it against a model free approach (Q-learning or DQN). We
then combine the 2 approaches, to guide the MCTS tree search with
a learned heuristic. We target a fast, safe and explainable solution.
We expect the safety and explainability to be obtained thanks to the
model based approach and to gain some speed up thanks to the model
free approach.

\subsection{MDP model}

A representation of the states in absolute coordinates would be $S_{t}=\left\{ \left(x,y,v_{x},v_{y}\right)_{\text{ego}},\left(x,y,v_{x},v_{y}\right)_{\text{obj}_{1..10}}\right\} $.
But we use a relative and normalized $\in\left[-1,1\right]$ representation
of the state (for easier generalization and learning) and account
for the fact that the ego car drives along the y-axis only\textbf{. }
\begin{itemize}
\item \textbf{States:} $S=\left\{ \left(\frac{y}{y^{max}},\frac{v_{y}}{v_{y}^{max}}\right)_{\text{ego}},\left(\frac{\Delta x}{\Delta x^{max}},\frac{\Delta y}{\Delta y^{max}},\frac{\Delta v_{x}}{\Delta v_{x}^{max}},\frac{\Delta v_{y}}{\Delta v_{y}^{max}}\right)_{\text{obj}_{1..10}}\right\} $
ie a vector $\in\mathbb{R}^{42}$ 
\end{itemize}
While the state space is continuous we use a discrete action space. 
\begin{itemize}
\item \textbf{Actions:} $A=\left[-2\;ms^{-2},-1\;ms^{-2},0\;ms^{-2},1\;ms^{-2},2\;ms^{-2}\right]$
corresponding to the longitudinal acceleration.\textbf{ }
\end{itemize}
The Transition model corresponds to standard linear Gaussian dynamics
with:
\begin{itemize}
\item \textbf{Transitions:} $T\left(s'\mid s,a\right)=P\left(S_{i}^{t+1}\mid S_{i}^{t},a\right)=\mathcal{N}\left(T_{s}S_{i}^{t}+T_{a}\begin{bmatrix}a_{x}\\
a_{y}
\end{bmatrix}\right)$ 
\end{itemize}
Using a Constant Velocity Model with $T_{s}=\begin{bmatrix}1 & 0 & \text{dt} & 0\\
0 & 1 & 0 & \text{dt}\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},S_{i}^{t+1}=\begin{bmatrix}x\\
y\\
v_{x}\\
v_{y}
\end{bmatrix}T_{a}=\begin{bmatrix}\frac{\text{dt}^{2}}{2} & 0\\
0 & \frac{\text{dt}^{2}}{2}\\
\text{dt} & 0\\
0 & \text{dt}
\end{bmatrix}$. The reward model accounts for efficiency (we penalize every step),
safety (we heavily pernalize collisions) and comfort (we penalize
strong accelerations and decelerations:
\begin{itemize}
\item \textbf{Reward}: $R(s,a)=-1-1000\times1\left[\text{d(ego,obj)}_{s}\leq10\right]-1\left[\left|a\right|=2\right]$
\end{itemize}

\subsection{Algo 1, MCTS tree search}

The MDP is solved online with MCTS tree search. Solving it offline
with Value Iteration is not an option as we are dealing with a huge
state space. MCTS is one of the most successfull sampling-based online
approaches used in recent years. It is the core part of AlphaGo Zero
\cite{DBLP:journals/corr/abs-1712-01815}. A description of the algorithm
is provided in \cite{Kochenderfer2015}. This algorithm involves running
many simulations from the current state while updating an estimate
of the state-action value function $Q(s,a)$ along its path of exploration.
Online algorithms enable to reduce the search space to the portion
of the state space that is reachable from a current state. MCTS has
the possibility to balance exploration and exploitation typically
via a method called Upper Confidence Bound: during the search we execute
the action that maximizes $Q(s,a)+c\sqrt{\frac{\log N(s)}{N(s,a)}}$
where $N(s),N(s,a)$ track the number of times a state and state-action
pair are visited . $c$ is a parameter that controls the amount of
exploration in the search: it will encourage exploring less visited
$(s,a)$ pairs and rely on the learned policy via $Q(s,a)$ estimates
for pairs that are well expolored, to choose an action from. Once
we reach a state that is not part of the explored set, we iterate
over all possible actions from that state and expand the tree. After
the expansion stage, a rollout is performed: the rollout consists
in running many random simulations till we reach some depth. It is
a Monte Carlo estimate of a state so the rollout policy are typically
stochastic and do not have to be close to optimal. The rollout policy
is different than the policy used for exploitation/exploration presented
above. Simulations, running from the root of the tree down to a leaf
node expansion, followed by a rollout evaluation phase, are run until
some stopping criterion is met: a time limit or a maximum number of
iterations. We then execute the action that maximizes $Q(s,a)$ at
the root of the tree. The pseudo code of the algorithm is provided
below:

{\scriptsize{}\begin{algorithmic}[1] 
\Function{SelectAction}{$s,d$} 
	\Loop
		\State \Call {Simulate}{$s,d,\pi_0$}
	\EndLoop
	\State \Return arg max$_a\text{ }Q(s,a)$
\EndFunction
\end{algorithmic}}{\scriptsize\par}

{\scriptsize{}\begin{algorithmic}[1] 

\Function{Simulate}{$s,d,\pi_0$} 
	\If {$d=0$}
		\State \Return $0$
	\EndIf

	\If {$s \notin T$} 
		\For {$a \in A(s)$}
			\State $(N(s,a),Q(s,a)) \gets (N_0(s,a),Q_0(s,a))$
		\EndFor
		\State $T=T \cup \{s\}$
		\State \Return \Call {Rollout}{$s,d,\pi_0$} 
	\EndIf

	\State $a \gets \text{arg max}_a\text{ }Q(s,a)+c\sqrt{\frac{logN(s)}{N(s,a)}}$ 
	\State $(s',r) \sim G(s,a)$
	\State $q \gets r+\lambda$ \Call {Simulate}{$s,d-1,\pi_0$}
	\State $N(s,a) \gets N(s,a)+1$
	\State $Q(s,a) \gets Q(s,a)+ \frac{q-Q(s,a)}{N(s,a)}$
	\State \Return $q$ 
\EndFunction

\end{algorithmic}}{\scriptsize\par}

{\scriptsize{}\begin{algorithmic}[1] 

\Function{Rollout}{$s,d,\pi_0$} 
	\If {$d=0$}
		\State \Return $0$
	\EndIf
	\State $a \sim \pi_0(s)$
	\State $(s',r) \sim G(s,a)$
	\State \Return $r+\lambda$ \Call {Rollout}{$s',d-1,\pi_0$}
\EndFunction
\end{algorithmic}}{\scriptsize\par}

One remaining problem is that in chess or go, state space is discrete
and the above algorithm does not cope with continuous state space:
the same state may never be sampled more than once from the generative
model which will result in a shallow tree with just one layer. The
Progressive Widening variant of MCTS \cite{DBLP:journals/corr/abs-1709-06196,Couetoux:2011:CUC:2177360.2177393}
solves this problem by controlling the sampling of new states and
the sampling among already existing states to enable exploration in
depth and not just in breadth. 

\subsection{Algo 2, Approximate Q-learning}

While as in \cite{Silver2017MasteringTG,DBLP:journals/corr/abs-1905-12197,DBLP:journals/corr/abs-1905-02680,DBLP:journals/corr/abs-1712-01815,8814125}
we intend to use Deep Learning to learn an evaluation function that
will be used later on as a heuristic to guide the MCTS tree search,
we will first try Approximate Q-learning to check how it performs
with a simple set of handcrafted features. We first derive a simple
features extractor. $\phi(s,a)=\begin{bmatrix}s_{6\times1}\\
a_{1\times1}\\
s_{6\times1}^{2}\\
a_{1\times1}^{2}
\end{bmatrix}$ where we take into account the state vector of the ego car (2 components)
and the state vector of the car with the smallest TTC (4 components).
We also take into account the acceleration command of the ego car.
We use quadratic components as well: as it is expected that the value
of a $(s,a)$ tuple will depend on distances computations. Typically
when computing Time To Collision, quadratic terms appear so we want
to provide these relevant features and figure out by learning the
weights associated to these features. We try to focus on a reduced
set of relevant features to speed up training. The Q-function is parametrized
by a vector $w$ with $\hat{Q}_{\text{opt}}(s,a;\mathbf{w})=\mathbf{w}\cdot\phi(s,a)$.
We have $\hat{V}_{\text{opt}}(s')=\underset{a'\in\text{Actions}(s')}{\max}\hat{Q}_{\text{opt}}(s',a')$
and use the objective $\left(\hat{Q}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right)^{2}$
which leads to the following update rule while performing Sochastic
Gradient Descent: $\mathbf{w}\leftarrow\mathbf{w}-\eta\left[\hat{Q}_{\text{opt}}(s,a;\mathbf{w})_{\text{pred}}-{\color{green}{\normalcolor \left(r+\gamma\hat{V}_{\text{opt}}(s')\right)_{\text{targ}}}}\right]\phi(s,a)$.
One of the problem we may encounter is that the data in simulation
is not iid, but highly correlated from one simulation step to the
other and the targets will vary a lot. This problem is typically handled
by using an experience replay buffer (which is possible with an off
policy algorithm) and using a different fixed Q-network for targets
evaluation: which is updated less frequently than the Q-function used
for predictions: as described in DeepMind papers \cite{article,DBLP:journals/corr/MnihKSGAWR13}. 

\subsection{Algo 3, Deep Q-learning}

We use a DQN algorithm with a replay memory buffer to ensure we are
dealing with iid samples and a target network, updated less frequently
than the optimisation network, to stabilize the training procedure
as described in DeepMind papers \cite{article,DBLP:journals/corr/MnihKSGAWR13}.
We use a Huber Loss which acts as MSE when error is small and as a
mean absolute error when error is large: to make it more robust to
outliers when the estimates of $Q$ are very noisy. Exploration is
done with an $\epsilon$ -greedy policy. A batch size of $128$ is
used, $\gamma=0.999$ and an Adam optimizer is used for the Batch
Gradient Descent updates. Programming is done with pytorch and we
use this code \href{https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html}{pytorch.org}
as a starting point for the DQN setup.

Our Neural Network has 42 neurons as input, corresponding to the state
vector, and 5 neurons as output. We will use a Neural Network architecture
slightly adapted from \cite{DBLP:journals/corr/abs-1803-10056}. It
is based on a CNN network as we want to have translational invariance
of the input. It should not matter to provide information about different
cars in one order or the other.
\begin{center}
\includegraphics[scale=0.3]{img/CNN_for_DQN}
\par\end{center}

\subsection{Algo 4, MCTS tree search with a learned heuristic }

We plan to use our learned Q-network $\hat{Q}(s,a;\mathbf{w})$ via
approximate Q-learning or Deep Q-learning as a heuristic for MCTS
tree search: to expand the tree in the most promising areas and hence
come up faster with a good solution. A solution is considered good
as soon as it is estimated collision free; we may run further MCTS
tree searches up to some time limit, to find even better solutions:
faster or more comfortable.

\section{Experiments}

The source code is available here: \href{https://github.com/PhilippeW83440/CS221_Project}{CS221 Project}.
The baseline (simple rule - reflex based) and oracle (assuming no
uncertainty using UCS/A{*} tree search) have been implemented at the
proposal stage. For the progress report, we have a first version of
Q-learning with some results summarized in the appendix. We are working
on the MCTS implementation.

\nocite{*}
\printbibliography

\end{document}
